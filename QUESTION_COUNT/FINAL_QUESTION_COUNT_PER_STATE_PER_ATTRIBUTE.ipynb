{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9852a6b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] --input INPUT [INPUT ...] --outdir OUTDIR\n",
      "                             [--dedup-col DEDUP_COL]\n",
      "ipykernel_launcher.py: error: the following arguments are required: --input/-i, --outdir/-o\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anjalisingh/Library/Python/3.13/lib/python/site-packages/IPython/core/interactiveshell.py:3678: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "state_attribute_counts_fixed.py\n",
    "Improved production-ready script to compute question counts per state per attribute\n",
    "from CSVs containing graph_path data.\n",
    "\n",
    "Usage:\n",
    "  python state_attribute_counts_fixed.py --input \"/path/Full Dataset.csv\" --outdir \"/path/output\"\n",
    "\"\"\"\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "from typing import List, Optional, Set\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# ------------------------\n",
    "# Logging config\n",
    "# ------------------------\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(levelname)s | %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# ------------------------\n",
    "# Helper functions\n",
    "# ------------------------\n",
    "def safe_read_csv(path: str) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"Read CSV with fallbacks for common encoding/engine issues.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "        logger.info(\"Loaded %d rows from %s\", len(df), path)\n",
    "        return df\n",
    "    except Exception as e1:\n",
    "        logger.warning(\"pd.read_csv failed: %s ‚Äî trying engine='python' with utf-8-sig\", e1)\n",
    "        try:\n",
    "            df = pd.read_csv(path, engine=\"python\", encoding=\"utf-8-sig\", on_bad_lines='skip')\n",
    "            logger.info(\"Loaded %d rows (fallback) from %s\", len(df), path)\n",
    "            return df\n",
    "        except Exception as e2:\n",
    "            logger.error(\"Failed to read %s: %s\", path, e2)\n",
    "            return None\n",
    "\n",
    "\n",
    "def find_graph_columns(df: pd.DataFrame) -> List[str]:\n",
    "    \"\"\"Return list of columns that look like graph_path or graph_path/0 etc.\"\"\"\n",
    "    cols = df.columns.tolist()\n",
    "    candidates = [c for c in cols if re.search(r'graph|path', c, flags=re.I)]\n",
    "    if any(c.lower() == \"graph_path\" for c in candidates):\n",
    "        return [\"graph_path\"]\n",
    "    return sorted(candidates)\n",
    "\n",
    "\n",
    "def merge_graph_columns(df: pd.DataFrame, graph_cols: List[str]) -> pd.Series:\n",
    "    \"\"\"Merge multiple graph columns into a single combined_graph string per row.\"\"\"\n",
    "    if not graph_cols:\n",
    "        return pd.Series([\"\"] * len(df), index=df.index)\n",
    "    if len(graph_cols) == 1:\n",
    "        return df[graph_cols[0]].astype(str).fillna(\"\")\n",
    "    \n",
    "    def join_row_parts(row):\n",
    "        parts = []\n",
    "        for c in graph_cols:\n",
    "            v = row.get(c)\n",
    "            if pd.isna(v):\n",
    "                continue\n",
    "            s = str(v).strip()\n",
    "            if s:\n",
    "                parts.append(s)\n",
    "        return \"|\".join(parts)\n",
    "\n",
    "    return df.apply(join_row_parts, axis=1)\n",
    "\n",
    "\n",
    "def extract_state_from_graph(text: str) -> Optional[str]:\n",
    "    \"\"\"Try to extract the state token from combined graph string.\"\"\"\n",
    "    if not isinstance(text, str) or not text:\n",
    "        return None\n",
    "    m = re.search(r'state[:=]([A-Za-z0-9_ \\-]+)', text, flags=re.I)\n",
    "    if m:\n",
    "        return m.group(1).strip().title()\n",
    "    return None\n",
    "\n",
    "\n",
    "def detect_attributes_from_graphs(series: pd.Series) -> Set[str]:\n",
    "    \"\"\"Scan combined_graph series and return set of attribute keys (left of ':').\"\"\"\n",
    "    attrs = set()\n",
    "    for text in series.dropna().astype(str):\n",
    "        tokens = re.split(r'\\||;', text)\n",
    "        for t in tokens:\n",
    "            t = t.strip()\n",
    "            if not t or ':' not in t:\n",
    "                continue\n",
    "            key = t.split(':', 1)[0].strip().lower()\n",
    "            if key and key != \"state\":\n",
    "                attrs.add(key)\n",
    "    return attrs\n",
    "\n",
    "\n",
    "def make_attribute_flags(df: pd.DataFrame, combined_graph_col: str, attributes: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"Create 0/1 columns in df for each attribute.\"\"\"\n",
    "    for att in attributes:\n",
    "        pattern = rf'(?i)\\b{re.escape(att)}:'\n",
    "        df[att] = df[combined_graph_col].astype(str).str.contains(pattern, regex=True, na=False).astype(int)\n",
    "    return df\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# Core processing\n",
    "# ------------------------\n",
    "def process_file(input_path: str, outdir: str, dedup_col: Optional[str] = \"Corrected Question\"):\n",
    "    logger.info(\"Processing file: %s\", input_path)\n",
    "    df = safe_read_csv(input_path)\n",
    "    if df is None:\n",
    "        raise RuntimeError(f\"Failed to load {input_path}\")\n",
    "\n",
    "    graph_cols = find_graph_columns(df)\n",
    "    if not graph_cols:\n",
    "        raise RuntimeError(f\"No graph-like column found in {input_path}\")\n",
    "\n",
    "    logger.info(\"Found graph columns: %s\", graph_cols)\n",
    "    df[\"__combined_graph\"] = merge_graph_columns(df, graph_cols)\n",
    "\n",
    "    # Extract state\n",
    "    df[\"__state_extracted\"] = df[\"__combined_graph\"].apply(extract_state_from_graph)\n",
    "    state_col_candidates = [c for c in df.columns if c.lower().strip() == \"state\"]\n",
    "    df[\"__state_fallback\"] = df[state_col_candidates[0]].astype(str).str.strip().replace({\"nan\": \"\"}) if state_col_candidates else \"\"\n",
    "\n",
    "    def pick_state(row):\n",
    "        if row[\"__state_extracted\"]:\n",
    "            return row[\"__state_extracted\"]\n",
    "        if row[\"__state_fallback\"]:\n",
    "            return row[\"__state_fallback\"].title()\n",
    "        return \"UNKNOWN\"\n",
    "\n",
    "    df[\"state_final\"] = df.apply(pick_state, axis=1)\n",
    "\n",
    "    # Detect attributes\n",
    "    detected_attrs = detect_attributes_from_graphs(df[\"__combined_graph\"])\n",
    "    attributes = sorted(list(detected_attrs)) if detected_attrs else [\"tourism\", \"history\", \"art\", \"festival\", \"cuisine\", \"personalities\", \"costume\"]\n",
    "    logger.info(\"Using attributes: %s\", attributes)\n",
    "\n",
    "    # Create flags\n",
    "    df = make_attribute_flags(df, \"__combined_graph\", attributes)\n",
    "\n",
    "    # Raw counts grouped by state\n",
    "    group = df.groupby(\"state_final\")[attributes].sum().reset_index().rename(columns={\"state_final\": \"state\"})\n",
    "    group[\"Total_Questions\"] = group[attributes].sum(axis=1).astype(int)\n",
    "\n",
    "    # Save raw counts\n",
    "    base_name = os.path.splitext(os.path.basename(input_path))[0]\n",
    "    raw_out = os.path.join(outdir, f\"{base_name}_state_attribute_counts_raw.csv\")\n",
    "    group.to_csv(raw_out, index=False)\n",
    "    logger.info(\"Saved raw state-attribute counts to %s\", raw_out)\n",
    "\n",
    "    # Unique counts if dedup_col exists\n",
    "    unique_out = None\n",
    "    if dedup_col in df.columns:\n",
    "        df_unique = df.dropna(subset=[dedup_col]).drop_duplicates(subset=[dedup_col])\n",
    "        group_unique = df_unique.groupby(\"state_final\")[attributes].sum().reset_index().rename(columns={\"state_final\": \"state\"})\n",
    "        group_unique[\"Total_Questions\"] = group_unique[attributes].sum(axis=1).astype(int)\n",
    "        unique_out = os.path.join(outdir, f\"{base_name}_state_attribute_counts_unique.csv\")\n",
    "        group_unique.to_csv(unique_out, index=False)\n",
    "        logger.info(\"Saved unique-question counts to %s\", unique_out)\n",
    "    else:\n",
    "        logger.info(\"Deduplication column '%s' not found; skipping unique counts.\", dedup_col)\n",
    "\n",
    "    # Save percent-augmented file\n",
    "    pct_df = group.copy()\n",
    "    for att in attributes:\n",
    "        pct_df[f\"{att}_pct\"] = (pct_df[att] / pct_df[\"Total_Questions\"].replace({0: 1})) * 100\n",
    "    pct_out = os.path.join(outdir, f\"{base_name}_state_attribute_counts_with_percent.csv\")\n",
    "    pct_df.to_csv(pct_out, index=False)\n",
    "    logger.info(\"Saved percent-augmented file to %s\", pct_out)\n",
    "\n",
    "    return {\"raw\": raw_out, \"unique\": unique_out, \"percent\": pct_out, \"attributes\": attributes}\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# CLI main\n",
    "# ------------------------\n",
    "def main(argv: Optional[List[str]] = None):\n",
    "    parser = argparse.ArgumentParser(description=\"Compute question counts per state per attribute from CSVs with graph_path.\")\n",
    "    parser.add_argument(\"--input\", \"-i\", nargs=\"+\", required=True, help=\"Input CSV file(s).\")\n",
    "    parser.add_argument(\"--outdir\", \"-o\", required=True, help=\"Output directory for CSV summary files.\")\n",
    "    parser.add_argument(\"--dedup-col\", \"-d\", default=\"Corrected Question\", help=\"Column to use for unique-question deduplication.\")\n",
    "    args = parser.parse_args(argv)\n",
    "\n",
    "    outdir = args.outdir\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "    results = {}\n",
    "    for input_path in args.input:\n",
    "        if not os.path.isfile(input_path):\n",
    "            logger.error(\"Input file not found: %s\", input_path)\n",
    "            continue\n",
    "        try:\n",
    "            info = process_file(input_path, outdir, dedup_col=args.dedup_col)\n",
    "            results[input_path] = info\n",
    "        except Exception as e:\n",
    "            logger.exception(\"Failed processing %s: %s\", input_path, e)\n",
    "\n",
    "    logger.info(\"DONE. Generated outputs for %d files.\", len(results))\n",
    "    if not results:\n",
    "        logger.error(\"No outputs generated. Check input file paths and format.\")\n",
    "        sys.exit(2)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c4e6181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded Full Dataset - Final Dataset.csv: 3284 rows, 9 columns\n",
      "‚úÖ Loaded Full Dataset - 3_hop_questions.csv: 155 rows, 11 columns\n",
      "üì¶ Combined dataset: 3380 rows, 18 columns\n",
      "Detected potential graph/support columns: ['supporting_facts', 'graph_path', 'supporting_facts/0', 'supporting_facts/1', 'graph_path/0', 'graph_path/1', 'graph_path/2', 'graph_path/3']\n",
      "‚úÖ Saved combined state-attribute question count to:\n",
      "/Users/anjalisingh/Desktop/IITP/QUESTION_COUNT/Combined_State_Attribute_Question_Count.csv\n",
      "\n",
      "üìä SAMPLE OUTPUT:\n",
      "            state  art  costume  cuisine  festival  history  medicine  personalities  religion  sports  tourism  total_questions\n",
      "        telangana   97       48       16         0      450         0            268        55      14      347              667\n",
      "       tamil_nadu   39       58       55       121      115        32              5        69       0       40              267\n",
      "        karnataka   19       13       75       146      109         0              9         0      14       29              236\n",
      "            assam   70       73       60        49       90         0              0        21       0       79              221\n",
      "        jharkhand   34        0       11        48      142         0             22         0       0       39              200\n",
      "           sikkim   97        0        0         0       35         0              0        40      39      161              188\n",
      "      west_bengal   69       72       56        85       16         0              0         5       0        9              156\n",
      "arunachal_pradesh   59        0        0        97        0         0              0         0       0       66              139\n",
      "            bihar   80        0        0        26       34         0              0        39       0       98              139\n",
      " himachal_pradesh   27       15       34        64       62         0              7         0       0       45              131\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Question Count per State per Attribute (Combined Datasets)\n",
    "# -------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "# -------------------------------\n",
    "# 1. CONFIG: Paths to datasets\n",
    "# -------------------------------\n",
    "dataset_final = \"/Users/anjalisingh/Desktop/IITP/QUESTION_COUNT/Full Dataset - Final Dataset.csv\"\n",
    "dataset_3hop = \"/Users/anjalisingh/Desktop/IITP/QUESTION_COUNT/Full Dataset - 3_hop_questions.csv\"\n",
    "output_dir = os.path.dirname(dataset_final)\n",
    "output_path = os.path.join(output_dir, \"Combined_State_Attribute_Question_Count.csv\")\n",
    "\n",
    "# -------------------------------\n",
    "# 2. LOAD DATASETS\n",
    "# -------------------------------\n",
    "def load_csv(path):\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "        print(f\"‚úÖ Loaded {os.path.basename(path)}: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        raise FileNotFoundError(f\"‚ùå Error loading {path}: {e}\")\n",
    "\n",
    "df_final = load_csv(dataset_final)\n",
    "df_3hop = load_csv(dataset_3hop)\n",
    "\n",
    "# Combine both datasets\n",
    "df = pd.concat([df_final, df_3hop], ignore_index=True)\n",
    "df = df.drop_duplicates(subset=['question'])  # optional, if same questions exist\n",
    "print(f\"üì¶ Combined dataset: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "\n",
    "# -------------------------------\n",
    "# 3. MERGE ALL GRAPH / SUPPORTING COLUMNS\n",
    "# -------------------------------\n",
    "# Detect relevant columns\n",
    "possible_cols = [c for c in df.columns if 'graph' in c.lower() or 'support' in c.lower()]\n",
    "print(f\"Detected potential graph/support columns: {possible_cols}\")\n",
    "\n",
    "# Combine into a single string per row\n",
    "df['combined_paths'] = df[possible_cols].astype(str).agg('|'.join, axis=1)\n",
    "\n",
    "# -------------------------------\n",
    "# 4. EXTRACT STATE AND ATTRIBUTES\n",
    "# -------------------------------\n",
    "def extract_info(text):\n",
    "    \"\"\"\n",
    "    Extract state and all attributes like tourism, history, art, festival, cuisine, costume, religion, sports, personalities, medicine\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return None, []\n",
    "\n",
    "    state_match = re.search(r'state:([a-zA-Z_\\s]+)', text.lower())\n",
    "    state = state_match.group(1).strip() if state_match else None\n",
    "\n",
    "    attrs = re.findall(r'(tourism|art|history|festival|cuisine|costume|religion|sports|personalities|medicine):', text.lower())\n",
    "    return state, attrs\n",
    "\n",
    "df[['state', 'attributes']] = df['combined_paths'].apply(lambda x: pd.Series(extract_info(x)))\n",
    "\n",
    "# Drop rows without state\n",
    "df = df.dropna(subset=['state'])\n",
    "\n",
    "# -------------------------------\n",
    "# 5. COUNT QUESTIONS PER STATE-ATTRIBUTE\n",
    "# -------------------------------\n",
    "rows = []\n",
    "for _, row in df.iterrows():\n",
    "    state = row['state']\n",
    "    for attr in row['attributes']:\n",
    "        rows.append({'state': state, 'attribute': attr})\n",
    "\n",
    "count_df = pd.DataFrame(rows)\n",
    "\n",
    "# Aggregate counts\n",
    "pivot_df = count_df.pivot_table(index='state', columns='attribute', aggfunc='size', fill_value=0)\n",
    "\n",
    "# Add total question count per state\n",
    "# Count unique questions per state\n",
    "unique_questions_per_state = df.groupby('state').size()\n",
    "pivot_df['total_questions'] = pivot_df.index.map(unique_questions_per_state)\n",
    "\n",
    "\n",
    "# Sort by total questions\n",
    "pivot_df = pivot_df.sort_values(by='total_questions', ascending=False)\n",
    "\n",
    "# Reset index for clean output\n",
    "pivot_df = pivot_df.reset_index()\n",
    "\n",
    "# -------------------------------\n",
    "# 6. SAVE + DISPLAY\n",
    "# -------------------------------\n",
    "pivot_df.to_csv(output_path, index=False)\n",
    "print(f\"‚úÖ Saved combined state-attribute question count to:\\n{output_path}\\n\")\n",
    "\n",
    "print(\"üìä SAMPLE OUTPUT:\")\n",
    "print(pivot_df.head(10).to_string(index=False))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "baf19edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] --input INPUT [INPUT ...] --outdir OUTDIR\n",
      "                             [--dedup-col DEDUP_COL]\n",
      "ipykernel_launcher.py: error: the following arguments are required: --input/-i, --outdir/-o\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anjalisingh/Library/Python/3.13/lib/python/site-packages/IPython/core/interactiveshell.py:3678: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "state_attribute_counts.py\n",
    "Production-ready script to compute question counts per state per attribute\n",
    "from CSVs containing graph_path data.\n",
    "\n",
    "Usage:\n",
    "  python state_attribute_counts.py --input \"/path/Full Dataset - Final Dataset.csv\" --outdir \"/path/output\"\n",
    "\"\"\"\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "from typing import List, Optional, Set\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# ------------------------\n",
    "# Logging config\n",
    "# ------------------------\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(levelname)s | %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# Helper functions\n",
    "# ------------------------\n",
    "def safe_read_csv(path: str) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"Read CSV with fallbacks for common encoding/engine issues.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "        logger.info(\"Loaded %d rows from %s\", len(df), path)\n",
    "        return df\n",
    "    except Exception as e1:\n",
    "        logger.warning(\"pd.read_csv failed: %s â€” trying engine='python' with utf-8\", e1)\n",
    "        try:\n",
    "            df = pd.read_csv(path, engine=\"python\", encoding=\"utf-8\", error_bad_lines=False)  # type: ignore\n",
    "            logger.info(\"Loaded %d rows (fallback) from %s\", len(df), path)\n",
    "            return df\n",
    "        except Exception as e2:\n",
    "            logger.error(\"Failed to read %s: %s\", path, e2)\n",
    "            return None\n",
    "\n",
    "\n",
    "def find_graph_columns(df: pd.DataFrame) -> List[str]:\n",
    "    \"\"\"Return list of columns that look like graph_path or graph_path/0 etc.\"\"\"\n",
    "    cols = df.columns.tolist()\n",
    "    candidates = [c for c in cols if re.search(r'graph', c, flags=re.I)]\n",
    "    # Prefer single 'graph_path' if exists\n",
    "    if any(c.lower() == \"graph_path\" for c in candidates):\n",
    "        return [\"graph_path\"]\n",
    "    # Otherwise return all graph-like columns (graph_path/0, graph_path/1, ...)\n",
    "    return sorted(candidates)\n",
    "\n",
    "\n",
    "def merge_graph_columns(df: pd.DataFrame, graph_cols: List[str]) -> pd.Series:\n",
    "    \"\"\"Merge multiple graph columns into a single combined_graph string per row.\"\"\"\n",
    "    if not graph_cols:\n",
    "        # no graph columns found\n",
    "        return pd.Series([\"\"] * len(df), index=df.index)\n",
    "\n",
    "    if len(graph_cols) == 1:\n",
    "        return df[graph_cols[0]].astype(str).fillna(\"\")\n",
    "\n",
    "    # join non-null, non-empty parts with '|' (preserve token separators)\n",
    "    def join_row_parts(row):\n",
    "        parts = []\n",
    "        for c in graph_cols:\n",
    "            v = row.get(c)\n",
    "            if pd.isna(v):\n",
    "                continue\n",
    "            s = str(v).strip()\n",
    "            if s:\n",
    "                parts.append(s)\n",
    "        return \"|\".join(parts)\n",
    "\n",
    "    return df.apply(join_row_parts, axis=1)\n",
    "\n",
    "\n",
    "def extract_state_from_graph(text: str) -> Optional[str]:\n",
    "    \"\"\"Try to extract the state token from combined graph string.\"\"\"\n",
    "    if not isinstance(text, str) or not text:\n",
    "        return None\n",
    "    # Look for state:VALUE tokens (allow letters, numbers, spaces, underscore, hyphen)\n",
    "    m = re.search(r'state:([A-Za-z0-9_ \\-]+)', text, flags=re.I)\n",
    "    if m:\n",
    "        # Normalize: strip and title-case\n",
    "        val = m.group(1).strip()\n",
    "        return val.title()\n",
    "    return None\n",
    "\n",
    "\n",
    "def detect_attributes_from_graphs(series: pd.Series) -> Set[str]:\n",
    "    \"\"\"Scan combined_graph series and return set of attribute keys (left of ':').\"\"\"\n",
    "    attrs = set()\n",
    "    for text in series.dropna().astype(str):\n",
    "        # split by '|' or ';'\n",
    "        tokens = re.split(r'\\||;', text)\n",
    "        for t in tokens:\n",
    "            t = t.strip()\n",
    "            if not t:\n",
    "                continue\n",
    "            if ':' in t:\n",
    "                key = t.split(':', 1)[0].strip().lower()\n",
    "                if key and key != \"state\":\n",
    "                    attrs.add(key)\n",
    "    return attrs\n",
    "\n",
    "\n",
    "def make_attribute_flags(df: pd.DataFrame, combined_graph_col: str, attributes: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"Create 0/1 columns in df for each attribute, based on presence in combined_graph_col.\"\"\"\n",
    "    for att in attributes:\n",
    "        # pattern: word boundary then attribute name then colon\n",
    "        pattern = rf'(?i)\\b{re.escape(att)}:'\n",
    "        df[att] = df[combined_graph_col].astype(str).str.contains(pattern, regex=True, na=False).astype(int)\n",
    "    return df\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# Core processing\n",
    "# ------------------------\n",
    "def process_file(input_path: str, outdir: str, dedup_col: Optional[str] = \"Corrected Question\"):\n",
    "    logger.info(\"Processing file: %s\", input_path)\n",
    "    df = safe_read_csv(input_path)\n",
    "    if df is None:\n",
    "        raise RuntimeError(f\"Failed to load {input_path}\")\n",
    "\n",
    "    # detect/merge graph columns\n",
    "    graph_cols = find_graph_columns(df)\n",
    "    if not graph_cols:\n",
    "        logger.error(\"No graph-like column found in %s. Please ensure your file has a 'graph_path' or similar column.\", input_path)\n",
    "        raise RuntimeError(\"Missing graph columns\")\n",
    "\n",
    "    logger.info(\"Found graph columns: %s\", graph_cols)\n",
    "    df[\"__combined_graph\"] = merge_graph_columns(df, graph_cols)\n",
    "\n",
    "    # extract state: prefer extracted state from graph_path; if not found, try existing 'state' column\n",
    "    df[\"__state_extracted\"] = df[\"__combined_graph\"].apply(extract_state_from_graph)\n",
    "\n",
    "    # if there's an explicit 'state' column, use it as fallback\n",
    "    state_col_candidates = [c for c in df.columns if c.lower().strip() == \"state\"]\n",
    "    if state_col_candidates:\n",
    "        used_state_col = state_col_candidates[0]\n",
    "        logger.info(\"Found explicit state column: %s (will be used as fallback)\", used_state_col)\n",
    "        df[\"__state_fallback\"] = df[used_state_col].astype(str).str.strip().replace({\"nan\": \"\"})\n",
    "    else:\n",
    "        df[\"__state_fallback\"] = \"\"\n",
    "\n",
    "    # final state: extracted if available, else fallback, else \"UNKNOWN\"\n",
    "    def pick_state(row):\n",
    "        if row[\"__state_extracted\"]:\n",
    "            return row[\"__state_extracted\"]\n",
    "        if row[\"__state_fallback\"]:\n",
    "            return row[\"__state_fallback\"].title()\n",
    "        return \"UNKNOWN\"\n",
    "\n",
    "    df[\"state_final\"] = df.apply(pick_state, axis=1)\n",
    "\n",
    "    # detect attributes automatically from the graph tokens\n",
    "    detected_attrs = detect_attributes_from_graphs(df[\"__combined_graph\"])\n",
    "    if not detected_attrs:\n",
    "        logger.warning(\"No attributes detected automatically. The file may not be formatted as expected.\")\n",
    "    attributes = sorted(list(detected_attrs))\n",
    "    logger.info(\"Auto-detected attributes (%d): %s\", len(attributes), attributes)\n",
    "\n",
    "    if not attributes:\n",
    "        # final fallback to common cultural attributes if nothing detected\n",
    "        attributes = [\"tourism\", \"history\", \"art\", \"festival\", \"cuisine\", \"personalities\", \"costume\"]\n",
    "        logger.info(\"Falling back to default attributes: %s\", attributes)\n",
    "\n",
    "    # create flags\n",
    "    df = make_attribute_flags(df, \"__combined_graph\", attributes)\n",
    "\n",
    "    # RAW counts (row-level) grouped by state\n",
    "    group = df.groupby(\"state_final\")[attributes].sum().reset_index().rename(columns={\"state_final\": \"state\"})\n",
    "    # compute total per state\n",
    "    group[\"Total_Questions\"] = group[attributes].sum(axis=1).astype(int)\n",
    "\n",
    "    # save raw counts\n",
    "    base_name = os.path.splitext(os.path.basename(input_path))[0]\n",
    "    raw_out = os.path.join(outdir, f\"{base_name}_state_attribute_counts_raw.csv\")\n",
    "    group.to_csv(raw_out, index=False)\n",
    "    logger.info(\"Saved raw state-attribute counts to %s\", raw_out)\n",
    "\n",
    "    # UNIQUE counts by Corrected Question if available\n",
    "    unique_out = None\n",
    "    if dedup_col in df.columns:\n",
    "        logger.info(\"Deduplicating by column: %s\", dedup_col)\n",
    "        df_unique = df.dropna(subset=[dedup_col]).drop_duplicates(subset=[dedup_col])\n",
    "        group_unique = df_unique.groupby(\"state_final\")[attributes].sum().reset_index().rename(columns={\"state_final\": \"state\"})\n",
    "        group_unique[\"Total_Questions\"] = group_unique[attributes].sum(axis=1).astype(int)\n",
    "        unique_out = os.path.join(outdir, f\"{base_name}_state_attribute_counts_unique.csv\")\n",
    "        group_unique.to_csv(unique_out, index=False)\n",
    "        logger.info(\"Saved unique-question state-attribute counts to %s\", unique_out)\n",
    "    else:\n",
    "        logger.info(\"Deduplication column '%s' not found; skipping unique-question counts.\", dedup_col)\n",
    "\n",
    "    # Also save raw counts with percentage columns (attr_pct per state)\n",
    "    pct_df = group.copy()\n",
    "    for att in attributes:\n",
    "        pct_df[f\"{att}_pct\"] = (pct_df[att] / pct_df[\"Total_Questions\"].replace({0: 1})) * 100\n",
    "    pct_out = os.path.join(outdir, f\"{base_name}_state_attribute_counts_with_percent.csv\")\n",
    "    pct_df.to_csv(pct_out, index=False)\n",
    "    logger.info(\"Saved percent-augmented file to %s\", pct_out)\n",
    "\n",
    "    # Return file paths for further use\n",
    "    return {\"raw\": raw_out, \"unique\": unique_out, \"percent\": pct_out, \"attributes\": attributes}\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# CLI main\n",
    "# ------------------------\n",
    "def main(argv: Optional[List[str]] = None):\n",
    "    parser = argparse.ArgumentParser(description=\"Compute question counts per state per attribute from CSVs with graph_path.\")\n",
    "    parser.add_argument(\"--input\", \"-i\", nargs=\"+\", required=True, help=\"Input CSV file(s).\")\n",
    "    parser.add_argument(\"--outdir\", \"-o\", required=True, help=\"Output directory for CSV summary files.\")\n",
    "    parser.add_argument(\"--dedup-col\", \"-d\", default=\"Corrected Question\", help=\"Column to use for unique-question deduplication.\")\n",
    "    args = parser.parse_args(argv)\n",
    "\n",
    "    outdir = args.outdir\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "    results = {}\n",
    "    for input_path in args.input:\n",
    "        if not os.path.isfile(input_path):\n",
    "            logger.error(\"Input file not found: %s\", input_path)\n",
    "            continue\n",
    "        try:\n",
    "            info = process_file(input_path, outdir, dedup_col=args.dedup_col)\n",
    "            results[input_path] = info\n",
    "        except Exception as e:\n",
    "            logger.exception(\"Failed processing %s: %s\", input_path, e)\n",
    "\n",
    "    logger.info(\"DONE. Generated outputs for %d files.\", len(results))\n",
    "    if not results:\n",
    "        logger.error(\"No outputs generated. Check input file paths and format.\")\n",
    "        sys.exit(2)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1785be3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”¹ Processing: Full Dataset - 3_hop_questions.csv\n",
      "âœ… Saved â†’ /Users/anjalisingh/Desktop/IITP/QUESTION_COUNT/state_attribute_outputs/Full Dataset - 3_hop_questions_state_attribute_summary.csv\n",
      "     state  tourism  art  history  festival  architecture  dance  music  \\\n",
      "0  Unknown       38   15       34         4             0      0      0   \n",
      "\n",
      "   literature  cuisine  heritage  language  religion  craft  handicraft  \\\n",
      "0           0        4         0         1         9      0           0   \n",
      "\n",
      "   costume  Total_Questions  \n",
      "0        2              107  \n",
      "\n",
      "ðŸ”¹ Processing: Full Dataset - Final Dataset.csv\n",
      "âœ… Saved â†’ /Users/anjalisingh/Desktop/IITP/QUESTION_COUNT/state_attribute_outputs/Full Dataset - Final Dataset_state_attribute_summary.csv\n",
      "               state  tourism  art  history  festival  architecture  dance  \\\n",
      "0    Andaman_Nicobar        1    0        0         0             0      0   \n",
      "1     Andhra_Pradesh       35   84       28        27             0      0   \n",
      "2  Arunachal_Pradesh       66   59        0        97             0      0   \n",
      "3              Assam       79   70       90        49             0      0   \n",
      "4              Bihar       96   77       33        26             0      0   \n",
      "\n",
      "   music  literature  cuisine  heritage  language  religion  craft  \\\n",
      "0      0           0        1         0         0         0      0   \n",
      "1      0           0       28         0         9         0      0   \n",
      "2      0           0        0         0        56         0      0   \n",
      "3      0           0       60         0         0        21      0   \n",
      "4      0           0        0         0         0        39      0   \n",
      "\n",
      "   handicraft  costume  Total_Questions  \n",
      "0           0        0                2  \n",
      "1           0       29              240  \n",
      "2           0        0              278  \n",
      "3           0       73              442  \n",
      "4           0        0              271  \n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------\n",
    "# âœ… Simplified Direct-Run Version (No Arguments Needed)\n",
    "# ---------------------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# âœ… Set your paths\n",
    "input_files = [\n",
    "    \"/Users/anjalisingh/Desktop/IITP/QUESTION_COUNT/Full Dataset - 3_hop_questions.csv\",\n",
    "    \"/Users/anjalisingh/Desktop/IITP/QUESTION_COUNT/Full Dataset - Final Dataset.csv\"\n",
    "]\n",
    "output_folder = \"/Users/anjalisingh/Desktop/IITP/QUESTION_COUNT/state_attribute_outputs\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "attributes = [\"tourism\", \"art\", \"history\", \"festival\", \"architecture\", \"dance\", \n",
    "              \"music\", \"literature\", \"cuisine\", \"heritage\", \"language\", \"religion\", \n",
    "              \"craft\", \"handicraft\", \"costume\"]\n",
    "\n",
    "def process_dataset(file_path):\n",
    "    print(f\"\\nðŸ”¹ Processing: {os.path.basename(file_path)}\")\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Detect graph_path-like columns\n",
    "    graph_col = [c for c in df.columns if \"graph\" in c.lower()][0]\n",
    "    df[\"state\"] = df[graph_col].astype(str).str.extract(r\"state:([a-zA-Z_]+)\").fillna(\"UNKNOWN\")\n",
    "    df[\"state\"] = df[\"state\"].str.title()\n",
    "\n",
    "    for att in attributes:\n",
    "        df[att] = df[graph_col].astype(str).str.contains(f\"{att}:\", case=False, na=False).astype(int)\n",
    "\n",
    "    result = df.groupby(\"state\")[attributes].sum().reset_index()\n",
    "    result[\"Total_Questions\"] = result[attributes].sum(axis=1)\n",
    "\n",
    "    out_path = os.path.join(output_folder, f\"{os.path.basename(file_path).split('.')[0]}_state_attribute_summary.csv\")\n",
    "    result.to_csv(out_path, index=False)\n",
    "    print(f\"âœ… Saved â†’ {out_path}\")\n",
    "    print(result.head())\n",
    "\n",
    "# Run for both files\n",
    "for file_path in input_files:\n",
    "    process_dataset(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a827570",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] --input INPUT [INPUT ...] --outdir OUTDIR\n",
      "                             [--dedup-col DEDUP_COL]\n",
      "ipykernel_launcher.py: error: the following arguments are required: --input/-i, --outdir/-o\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anjalisingh/Library/Python/3.13/lib/python/site-packages/IPython/core/interactiveshell.py:3678: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "state_attribute_counts_fixed.py\n",
    "Improved production-ready script to compute question counts per state per attribute\n",
    "from CSVs containing graph_path data.\n",
    "\n",
    "Usage:\n",
    "  python state_attribute_counts_fixed.py --input \"/path/Full Dataset.csv\" --outdir \"/path/output\"\n",
    "\"\"\"\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "from typing import List, Optional, Set\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# ------------------------\n",
    "# Logging config\n",
    "# ------------------------\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(levelname)s | %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# ------------------------\n",
    "# Helper functions\n",
    "# ------------------------\n",
    "def safe_read_csv(path: str) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"Read CSV with fallbacks for common encoding/engine issues.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "        logger.info(\"Loaded %d rows from %s\", len(df), path)\n",
    "        return df\n",
    "    except Exception as e1:\n",
    "        logger.warning(\"pd.read_csv failed: %s â€” trying engine='python' with utf-8-sig\", e1)\n",
    "        try:\n",
    "            df = pd.read_csv(path, engine=\"python\", encoding=\"utf-8-sig\", on_bad_lines='skip')\n",
    "            logger.info(\"Loaded %d rows (fallback) from %s\", len(df), path)\n",
    "            return df\n",
    "        except Exception as e2:\n",
    "            logger.error(\"Failed to read %s: %s\", path, e2)\n",
    "            return None\n",
    "\n",
    "\n",
    "def find_graph_columns(df: pd.DataFrame) -> List[str]:\n",
    "    \"\"\"Return list of columns that look like graph_path or graph_path/0 etc.\"\"\"\n",
    "    cols = df.columns.tolist()\n",
    "    candidates = [c for c in cols if re.search(r'graph|path', c, flags=re.I)]\n",
    "    if any(c.lower() == \"graph_path\" for c in candidates):\n",
    "        return [\"graph_path\"]\n",
    "    return sorted(candidates)\n",
    "\n",
    "\n",
    "def merge_graph_columns(df: pd.DataFrame, graph_cols: List[str]) -> pd.Series:\n",
    "    \"\"\"Merge multiple graph columns into a single combined_graph string per row.\"\"\"\n",
    "    if not graph_cols:\n",
    "        return pd.Series([\"\"] * len(df), index=df.index)\n",
    "    if len(graph_cols) == 1:\n",
    "        return df[graph_cols[0]].astype(str).fillna(\"\")\n",
    "    \n",
    "    def join_row_parts(row):\n",
    "        parts = []\n",
    "        for c in graph_cols:\n",
    "            v = row.get(c)\n",
    "            if pd.isna(v):\n",
    "                continue\n",
    "            s = str(v).strip()\n",
    "            if s:\n",
    "                parts.append(s)\n",
    "        return \"|\".join(parts)\n",
    "\n",
    "    return df.apply(join_row_parts, axis=1)\n",
    "\n",
    "\n",
    "def extract_state_from_graph(text: str) -> Optional[str]:\n",
    "    \"\"\"Try to extract the state token from combined graph string.\"\"\"\n",
    "    if not isinstance(text, str) or not text:\n",
    "        return None\n",
    "    m = re.search(r'state[:=]([A-Za-z0-9_ \\-]+)', text, flags=re.I)\n",
    "    if m:\n",
    "        return m.group(1).strip().title()\n",
    "    return None\n",
    "\n",
    "\n",
    "def detect_attributes_from_graphs(series: pd.Series) -> Set[str]:\n",
    "    \"\"\"Scan combined_graph series and return set of attribute keys (left of ':').\"\"\"\n",
    "    attrs = set()\n",
    "    for text in series.dropna().astype(str):\n",
    "        tokens = re.split(r'\\||;', text)\n",
    "        for t in tokens:\n",
    "            t = t.strip()\n",
    "            if not t or ':' not in t:\n",
    "                continue\n",
    "            key = t.split(':', 1)[0].strip().lower()\n",
    "            if key and key != \"state\":\n",
    "                attrs.add(key)\n",
    "    return attrs\n",
    "\n",
    "\n",
    "def make_attribute_flags(df: pd.DataFrame, combined_graph_col: str, attributes: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"Create 0/1 columns in df for each attribute.\"\"\"\n",
    "    for att in attributes:\n",
    "        pattern = rf'(?i)\\b{re.escape(att)}:'\n",
    "        df[att] = df[combined_graph_col].astype(str).str.contains(pattern, regex=True, na=False).astype(int)\n",
    "    return df\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# Core processing\n",
    "# ------------------------\n",
    "def process_file(input_path: str, outdir: str, dedup_col: Optional[str] = \"Corrected Question\"):\n",
    "    logger.info(\"Processing file: %s\", input_path)\n",
    "    df = safe_read_csv(input_path)\n",
    "    if df is None:\n",
    "        raise RuntimeError(f\"Failed to load {input_path}\")\n",
    "\n",
    "    graph_cols = find_graph_columns(df)\n",
    "    if not graph_cols:\n",
    "        raise RuntimeError(f\"No graph-like column found in {input_path}\")\n",
    "\n",
    "    logger.info(\"Found graph columns: %s\", graph_cols)\n",
    "    df[\"__combined_graph\"] = merge_graph_columns(df, graph_cols)\n",
    "\n",
    "    # Extract state\n",
    "    df[\"__state_extracted\"] = df[\"__combined_graph\"].apply(extract_state_from_graph)\n",
    "    state_col_candidates = [c for c in df.columns if c.lower().strip() == \"state\"]\n",
    "    df[\"__state_fallback\"] = df[state_col_candidates[0]].astype(str).str.strip().replace({\"nan\": \"\"}) if state_col_candidates else \"\"\n",
    "\n",
    "    def pick_state(row):\n",
    "        if row[\"__state_extracted\"]:\n",
    "            return row[\"__state_extracted\"]\n",
    "        if row[\"__state_fallback\"]:\n",
    "            return row[\"__state_fallback\"].title()\n",
    "        return \"UNKNOWN\"\n",
    "\n",
    "    df[\"state_final\"] = df.apply(pick_state, axis=1)\n",
    "\n",
    "    # Detect attributes\n",
    "    detected_attrs = detect_attributes_from_graphs(df[\"__combined_graph\"])\n",
    "    attributes = sorted(list(detected_attrs)) if detected_attrs else [\"tourism\", \"history\", \"art\", \"festival\", \"cuisine\", \"personalities\", \"costume\"]\n",
    "    logger.info(\"Using attributes: %s\", attributes)\n",
    "\n",
    "    # Create flags\n",
    "    df = make_attribute_flags(df, \"__combined_graph\", attributes)\n",
    "\n",
    "    # Raw counts grouped by state\n",
    "    group = df.groupby(\"state_final\")[attributes].sum().reset_index().rename(columns={\"state_final\": \"state\"})\n",
    "    group[\"Total_Questions\"] = group[attributes].sum(axis=1).astype(int)\n",
    "\n",
    "    # Save raw counts\n",
    "    base_name = os.path.splitext(os.path.basename(input_path))[0]\n",
    "    raw_out = os.path.join(outdir, f\"{base_name}_state_attribute_counts_raw.csv\")\n",
    "    group.to_csv(raw_out, index=False)\n",
    "    logger.info(\"Saved raw state-attribute counts to %s\", raw_out)\n",
    "\n",
    "    # Unique counts if dedup_col exists\n",
    "    unique_out = None\n",
    "    if dedup_col in df.columns:\n",
    "        df_unique = df.dropna(subset=[dedup_col]).drop_duplicates(subset=[dedup_col])\n",
    "        group_unique = df_unique.groupby(\"state_final\")[attributes].sum().reset_index().rename(columns={\"state_final\": \"state\"})\n",
    "        group_unique[\"Total_Questions\"] = group_unique[attributes].sum(axis=1).astype(int)\n",
    "        unique_out = os.path.join(outdir, f\"{base_name}_state_attribute_counts_unique.csv\")\n",
    "        group_unique.to_csv(unique_out, index=False)\n",
    "        logger.info(\"Saved unique-question counts to %s\", unique_out)\n",
    "    else:\n",
    "        logger.info(\"Deduplication column '%s' not found; skipping unique counts.\", dedup_col)\n",
    "\n",
    "    # Save percent-augmented file\n",
    "    pct_df = group.copy()\n",
    "    for att in attributes:\n",
    "        pct_df[f\"{att}_pct\"] = (pct_df[att] / pct_df[\"Total_Questions\"].replace({0: 1})) * 100\n",
    "    pct_out = os.path.join(outdir, f\"{base_name}_state_attribute_counts_with_percent.csv\")\n",
    "    pct_df.to_csv(pct_out, index=False)\n",
    "    logger.info(\"Saved percent-augmented file to %s\", pct_out)\n",
    "\n",
    "    return {\"raw\": raw_out, \"unique\": unique_out, \"percent\": pct_out, \"attributes\": attributes}\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# CLI main\n",
    "# ------------------------\n",
    "def main(argv: Optional[List[str]] = None):\n",
    "    parser = argparse.ArgumentParser(description=\"Compute question counts per state per attribute from CSVs with graph_path.\")\n",
    "    parser.add_argument(\"--input\", \"-i\", nargs=\"+\", required=True, help=\"Input CSV file(s).\")\n",
    "    parser.add_argument(\"--outdir\", \"-o\", required=True, help=\"Output directory for CSV summary files.\")\n",
    "    parser.add_argument(\"--dedup-col\", \"-d\", default=\"Corrected Question\", help=\"Column to use for unique-question deduplication.\")\n",
    "    args = parser.parse_args(argv)\n",
    "\n",
    "    outdir = args.outdir\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "    results = {}\n",
    "    for input_path in args.input:\n",
    "        if not os.path.isfile(input_path):\n",
    "            logger.error(\"Input file not found: %s\", input_path)\n",
    "            continue\n",
    "        try:\n",
    "            info = process_file(input_path, outdir, dedup_col=args.dedup_col)\n",
    "            results[input_path] = info\n",
    "        except Exception as e:\n",
    "            logger.exception(\"Failed processing %s: %s\", input_path, e)\n",
    "\n",
    "    logger.info(\"DONE. Generated outputs for %d files.\", len(results))\n",
    "    if not results:\n",
    "        logger.error(\"No outputs generated. Check input file paths and format.\")\n",
    "        sys.exit(2)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

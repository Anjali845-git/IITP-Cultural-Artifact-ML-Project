{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0468ffd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing fileA -> /Users/anjalisingh/Desktop/IITP/WordCloud/Full Dataset - Final Dataset.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/spacy/pipeline/lemmatizer.py:188: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved entity frequency CSV to: /Users/anjalisingh/Desktop/IITP/WordCloud/output/fileA_named_entities.csv\n",
      "Saved wordcloud to: /Users/anjalisingh/Desktop/IITP/WordCloud/output/fileA_named_entities_wordcloud.png\n",
      "Processing fileB -> /Users/anjalisingh/Desktop/IITP/WordCloud/Full Dataset - 3_hop_questions.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/spacy/pipeline/lemmatizer.py:188: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved entity frequency CSV to: /Users/anjalisingh/Desktop/IITP/WordCloud/output/fileB_named_entities.csv\n",
      "Saved wordcloud to: /Users/anjalisingh/Desktop/IITP/WordCloud/output/fileB_named_entities_wordcloud.png\n",
      "Done. Check the output directory: /Users/anjalisingh/Desktop/IITP/WordCloud/output\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "generate_named_entity_wordclouds.py\n",
    "\n",
    "Purpose:\n",
    "  - Load two CSVs containing a \"Corrected Question(s)\" column (user provided file paths).\n",
    "  - Run spaCy NER on each question to extract named entities.\n",
    "  - Clean / normalize extracted entities (remove stopwords, single chars, numbers, generic tokens).\n",
    "  - Produce wordcloud images for: 1) entities from file A, 2) entities from file B, 3) combined entities.\n",
    "  - Produce CSV reports with entity frequencies and top entities by label (PERSON, GPE, ORG, LOC, etc.).\n",
    "\n",
    "Usage:\n",
    "  - Edit the FILE_PATHS dictionary below to point to your local CSVs if different.\n",
    "  - Run: python3 generate_named_entity_wordclouds.py\n",
    "\n",
    "Outputs (saved to OUTPUT_DIR):\n",
    "  - fileA_named_entities.csv  (all entities + labels + frequencies)\n",
    "  - fileB_named_entities.csv\n",
    "  - combined_named_entities.csv\n",
    "  - fileA_named_entities_wordcloud.png\n",
    "  - fileB_named_entities_wordcloud.png\n",
    "  - combined_named_entities_wordcloud.png\n",
    "\n",
    "Notes / tips:\n",
    "  - The script will try to download NLTK stopwords and the spaCy \"en_core_web_sm\" model if missing.\n",
    "  - Make sure Python packages are installed: pandas, spacy, wordcloud, matplotlib, nltk\n",
    "    Example:\n",
    "      pip install pandas spacy wordcloud matplotlib nltk\n",
    "      python -m spacy download en_core_web_sm\n",
    "\n",
    "Author: ChatGPT (assistant)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import argparse\n",
    "import logging\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Try to import libraries and provide friendly error messages\n",
    "try:\n",
    "    import spacy\n",
    "except Exception as e:\n",
    "    print(\"spaCy is required. Install with: pip install spacy\")\n",
    "    raise\n",
    "\n",
    "try:\n",
    "    from wordcloud import WordCloud\n",
    "except Exception as e:\n",
    "    print(\"wordcloud is required. Install with: pip install wordcloud\")\n",
    "    raise\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    import nltk\n",
    "    from nltk.corpus import stopwords\n",
    "except Exception as e:\n",
    "    print(\"nltk is required. Install with: pip install nltk\")\n",
    "    raise\n",
    "\n",
    "# -------------------------------\n",
    "# CONFIG - edit these paths if needed\n",
    "# -------------------------------\n",
    "FILE_PATHS = {\n",
    "    \"fileA\": r\"/Users/anjalisingh/Desktop/IITP/WordCloud/Full Dataset - Final Dataset.csv\",\n",
    "    \"fileB\": r\"/Users/anjalisingh/Desktop/IITP/WordCloud/Full Dataset - 3_hop_questions.csv\",\n",
    "}\n",
    "\n",
    "OUTPUT_DIR = r\"/Users/anjalisingh/Desktop/IITP/WordCloud/output\"\n",
    "# Column name (as provided in your examples)\n",
    "# Accept both variants present in your files (3-hop uses \"Corrected Questions\")\n",
    "QUESTION_COLUMN_CANDIDATES = [\"Corrected Question\", \"Corrected Questions\"]  # order: final dataset, 3-hop\n",
    "\n",
    "\n",
    "# Entities to keep â€” spaCy labels we care most about\n",
    "KEEP_LABELS = {\"PERSON\", \"GPE\", \"LOC\", \"ORG\", \"NORP\", \"FAC\", \"PRODUCT\", \"EVENT\", \"WORK_OF_ART\"}\n",
    "\n",
    "# Additional tokens to drop (lowercase)\n",
    "EXTRA_STOPWORDS = {\n",
    "    \"which\", \"what\", \"where\", \"when\", \"who\", \"that\", \"same\", \"state\", \"india\",\n",
    "    \"indian\", \"associated\", \"located\", \"located\", \"famous\", \"known\",\n",
    "    \"whichis\", \"etc\", \"etc.\"\n",
    "}\n",
    "\n",
    "# -------------------------------\n",
    "# Helpers\n",
    "# -------------------------------\n",
    "\n",
    "def ensure_output_dir(path):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "\n",
    "def try_download_resources():\n",
    "    # NLTK stopwords\n",
    "    try:\n",
    "        nltk.data.find(\"corpora/stopwords\")\n",
    "    except LookupError:\n",
    "        print(\"Downloading NLTK stopwords...\")\n",
    "        nltk.download(\"stopwords\")\n",
    "\n",
    "    # spaCy model\n",
    "    try:\n",
    "        spacy.load(\"en_core_web_sm\")\n",
    "    except OSError:\n",
    "        print(\"Downloading spaCy 'en_core_web_sm' model (this may take a moment)...\")\n",
    "        from spacy.cli import download\n",
    "\n",
    "        download(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "def load_csv_questions(path, question_cols=QUESTION_COLUMN_CANDIDATES):\n",
    "    \"\"\"\n",
    "    Load CSV and return list of questions and the dataframe. Supports multiple possible column names.\n",
    "    \"\"\"\n",
    "    # Try a few encodings and fallback strategies\n",
    "    encodings = [\"utf-8\", \"ISO-8859-1\", \"latin1\"]\n",
    "    for enc in encodings:\n",
    "        try:\n",
    "            df = pd.read_csv(path, encoding=enc)\n",
    "            break\n",
    "        except Exception:\n",
    "            df = None\n",
    "    if df is None:\n",
    "        raise ValueError(f\"Could not read CSV at {path} with tried encodings: {encodings}\")\n",
    "\n",
    "    # Normalize column names (strip)\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "    # Try to find a matching question column from the candidates list\n",
    "    found_col = None\n",
    "    lower_map = {c.lower(): c for c in df.columns}\n",
    "    for qc in question_cols:\n",
    "        if qc in df.columns:\n",
    "            found_col = qc\n",
    "            break\n",
    "        if qc.lower() in lower_map:\n",
    "            found_col = lower_map[qc.lower()]\n",
    "            break\n",
    "\n",
    "    if not found_col:\n",
    "        raise KeyError(f\"None of the expected question columns {question_cols} found in {path}. Available columns: {df.columns.tolist()}\")\n",
    "\n",
    "    # Dropna and ensure strings\n",
    "    questions = df[found_col].dropna().astype(str).tolist()\n",
    "    return questions, df\n",
    "\n",
    "\n",
    "def clean_entity_text(text):\n",
    "    # Normalize whitespace and punctuation, keep basic tokens\n",
    "    text = text.strip()\n",
    "    # Remove leading/trailing punctuation\n",
    "    text = re.sub(r\"^[^\\w]+|[^\\w]+$\", \"\", text)\n",
    "    # Replace newlines and multiple spaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def is_useful_entity(ent_text):\n",
    "    if not ent_text:\n",
    "        return False\n",
    "    ent = ent_text.strip()\n",
    "    # remove single characters and purely numeric tokens\n",
    "    if len(ent) <= 1:\n",
    "        return False\n",
    "    if ent.isnumeric():\n",
    "        return False\n",
    "    # remove tokens that are just punctuation\n",
    "    if all(not ch.isalnum() for ch in ent):\n",
    "        return False\n",
    "    # remove generic tokens\n",
    "    low = ent.lower()\n",
    "    if low in EXTRA_STOPWORDS:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def extract_entities_from_questions(nlp, questions, keep_labels=KEEP_LABELS):\n",
    "    entities = []\n",
    "    label_map = []\n",
    "    # process in batches for speed\n",
    "    for doc in nlp.pipe(questions, disable=[\"parser\", \"tagger\"]):\n",
    "        for ent in doc.ents:\n",
    "            label = ent.label_\n",
    "            if keep_labels and label not in keep_labels:\n",
    "                # we can still keep some GPE/LOC/ORG etc; otherwise skip\n",
    "                continue\n",
    "            text = clean_entity_text(ent.text)\n",
    "            if is_useful_entity(text):\n",
    "                entities.append(text)\n",
    "                label_map.append((text, label))\n",
    "    return entities, label_map\n",
    "\n",
    "\n",
    "def normalize_entity_key(e):\n",
    "    # Lowercase but keep capitalization for appearance if needed.\n",
    "    # Here, we return a canonical lowercase key, but preserve original form later if required.\n",
    "    return e.lower()\n",
    "\n",
    "\n",
    "def build_frequency_counters(entities):\n",
    "    freq = Counter()\n",
    "    for e in entities:\n",
    "        k = normalize_entity_key(e)\n",
    "        freq[k] += 1\n",
    "    return freq\n",
    "\n",
    "\n",
    "def save_top_entities_csv(freq_counter, label_pairs, out_csv_path, top_n=200):\n",
    "    # label_pairs is list of (entity_original_text, label)\n",
    "    # We'll compute the most common tokens and their labels (most frequent label for that key)\n",
    "    label_counter = defaultdict(Counter)\n",
    "    for orig, label in label_pairs:\n",
    "        key = normalize_entity_key(orig)\n",
    "        label_counter[key][label] += 1\n",
    "\n",
    "    rows = []\n",
    "    for ent_key, count in freq_counter.most_common(top_n):\n",
    "        most_common_label = None\n",
    "        if ent_key in label_counter:\n",
    "            most_common_label = label_counter[ent_key].most_common(1)[0][0]\n",
    "        rows.append((ent_key, most_common_label or \"\", count))\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=[\"entity\", \"label\", \"count\"])\n",
    "    df.to_csv(out_csv_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "def generate_and_save_wordcloud(freq_counter, out_path, title=None, max_words=300):\n",
    "    # Filter very short keys (just in case) and convert keys back to display form\n",
    "    filtered = {k: v for k, v in freq_counter.items() if len(k) > 1 and not k.isnumeric()}\n",
    "    if not filtered:\n",
    "        print(f\"No tokens to render for {out_path}\")\n",
    "        return\n",
    "\n",
    "    wc = WordCloud(width=1200, height=600, background_color=\"white\", collocations=False, max_words=max_words)\n",
    "    wc.generate_from_frequencies(filtered)\n",
    "\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    if title:\n",
    "        plt.title(title, fontsize=18)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Main\n",
    "# -------------------------------\n",
    "\n",
    "def main():\n",
    "    ensure_output_dir(OUTPUT_DIR)\n",
    "    try_download_resources()\n",
    "\n",
    "    # load spaCy model\n",
    "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"tagger\"])  # we only need NER\n",
    "\n",
    "    # Build a combined stopword set\n",
    "    nltk_stop = set(stopwords.words('english'))\n",
    "    stop_words = set([w.lower() for w in nltk_stop]) | EXTRA_STOPWORDS\n",
    "\n",
    "    all_entities = []\n",
    "    all_labels = []\n",
    "\n",
    "    reports = {}\n",
    "\n",
    "    for key, path in FILE_PATHS.items():\n",
    "        print(f\"Processing {key} -> {path}\")\n",
    "        questions, df = load_csv_questions(path)\n",
    "        entities, label_pairs = extract_entities_from_questions(nlp, questions)\n",
    "\n",
    "        # Filter entities further using stopwords: remove entities that are just stopwords\n",
    "        entities = [e for e in entities if e.lower() not in stop_words]\n",
    "\n",
    "        freq = build_frequency_counters(entities)\n",
    "        reports[key] = {\n",
    "            \"entities\": entities,\n",
    "            \"label_pairs\": label_pairs,\n",
    "            \"freq\": freq,\n",
    "            \"df\": df,\n",
    "        }\n",
    "\n",
    "        # Save CSV report for this file\n",
    "        out_csv = os.path.join(OUTPUT_DIR, f\"{key}_named_entities.csv\")\n",
    "        save_top_entities_csv(freq, label_pairs, out_csv, top_n=1000)\n",
    "        print(f\"Saved entity frequency CSV to: {out_csv}\")\n",
    "\n",
    "        # Save wordcloud\n",
    "        out_png = os.path.join(OUTPUT_DIR, f\"{key}_named_entities_wordcloud.png\")\n",
    "        generate_and_save_wordcloud(freq, out_png, title=f\"Named Entities: {key}\")\n",
    "        print(f\"Saved wordcloud to: {out_png}\")\n",
    "\n",
    "        all_entities.extend(entities)\n",
    "        all_labels.extend(label_pairs)\n",
    "\n",
    "    # Combined\n",
    "    combined_freq = build_frequency_counters(all_entities)\n",
    "    save_top_entities_csv(combined_freq, all_labels, os.path.join(OUTPUT_DIR, \"combined_named_entities.csv\"), top_n=2000)\n",
    "    generate_and_save_wordcloud(combined_freq, os.path.join(OUTPUT_DIR, \"combined_named_entities_wordcloud.png\"), title=\"Combined Named Entities\")\n",
    "\n",
    "    print(\"Done. Check the output directory:\", OUTPUT_DIR)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

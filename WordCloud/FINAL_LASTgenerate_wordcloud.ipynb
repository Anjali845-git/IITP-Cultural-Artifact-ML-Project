{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53948740",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--input INPUT] [--output_dir OUTPUT_DIR]\n",
      "                             [--identifier_cols [IDENTIFIER_COLS ...]]\n",
      "                             [--artifact_col ARTIFACT_COL]\n",
      "                             [--exclude_names [EXCLUDE_NAMES ...]]\n",
      "                             [--top_k TOP_K]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --f=/Users/anjalisingh/Library/Jupyter/runtime/kernel-v3c837de481c8781c0b020b95ef94903aa168f27d4.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 2\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Production-ready script to generate wordclouds from a cultural artifacts dataset.\n",
    "\n",
    "Usage (CLI):\n",
    "    python wordcloud_generator_for_cultural_artifacts.py \\\n",
    "        --input \"/path/to/cultural-artifact-gold-sheet-final.csv\" \\\n",
    "        --output_dir \"/path/to/output/dir\" \\\n",
    "        --identifier_cols Identifier1 Identifier2\n",
    "\n",
    "Or in a Jupyter notebook: just run the script; it will auto-detect a CSV in cwd or\n",
    "create a tiny sample CSV to demonstrate functionality.\n",
    "\n",
    "Requirements:\n",
    "    pip install pandas spacy wordcloud matplotlib nltk tqdm\n",
    "    python -m spacy download en_core_web_sm\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import logging\n",
    "import glob\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib\n",
    "# Use non-interactive backend for headless environments\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# spaCy and NLTK\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --------------------------- Helper utilities ---------------------------\n",
    "\n",
    "def setup_logging():\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "        handlers=[logging.StreamHandler(sys.stdout)],\n",
    "    )\n",
    "\n",
    "\n",
    "def ensure_nltk_stopwords():\n",
    "    try:\n",
    "        nltk.data.find(\"corpora/stopwords\")\n",
    "    except LookupError:\n",
    "        logging.info(\"Downloading NLTK stopwords...\")\n",
    "        nltk.download(\"stopwords\")\n",
    "\n",
    "\n",
    "def load_spacy_model(model_name=\"en_core_web_sm\"):\n",
    "    try:\n",
    "        nlp = spacy.load(model_name)\n",
    "    except OSError:\n",
    "        logging.info(f\"spaCy model {model_name} not found. Attempting to download...\")\n",
    "        from spacy.cli import download\n",
    "\n",
    "        download(model_name)\n",
    "        nlp = spacy.load(model_name)\n",
    "    return nlp\n",
    "\n",
    "\n",
    "def sanitize_text(text: str) -> str:\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    # remove newlines and weird whitespace\n",
    "    return re.sub(r\"\\s+\", \" \", str(text)).strip()\n",
    "\n",
    "\n",
    "def detect_candidate_columns(df: pd.DataFrame):\n",
    "    \"\"\"Detect artifact column and identifier columns if not explicitly provided.\"\"\"\n",
    "    col_lower = {c.lower(): c for c in df.columns}\n",
    "\n",
    "    artifact_candidates = [\n",
    "        \"unique artifact\",\n",
    "        \"cultural artifact\",\n",
    "        \"artifact\",\n",
    "        \"unique_artifact\",\n",
    "        \"cultural artifact\",\n",
    "    ]\n",
    "    artifact_col = None\n",
    "    for cand in artifact_candidates:\n",
    "        if cand in col_lower:\n",
    "            artifact_col = col_lower[cand]\n",
    "            break\n",
    "\n",
    "    # identifier columns: any column whose name starts with 'identifier' (case-insensitive)\n",
    "    identifier_cols = [c for c in df.columns if c.lower().startswith(\"identifier\")]\n",
    "\n",
    "    # fallback: look for numbered identifier columns\n",
    "    if not identifier_cols:\n",
    "        for i in range(1, 6):\n",
    "            name = f\"identifier{i}\"\n",
    "            if name in col_lower:\n",
    "                identifier_cols.append(col_lower[name])\n",
    "\n",
    "    # final fallback: pick columns likely to contain short descriptor text\n",
    "    if not artifact_col:\n",
    "        possible = [c for c in df.columns if \"artifact\" in c.lower() or \"unique\" in c.lower()]\n",
    "        artifact_col = possible[0] if possible else df.columns[0]\n",
    "\n",
    "    return artifact_col, identifier_cols\n",
    "\n",
    "\n",
    "# --------------------------- NER + cleaning ---------------------------\n",
    "\n",
    "def extract_named_entities_from_text(nlp, text, accepted_labels=None):\n",
    "    \"\"\"Return a list of extracted entity strings from text using spaCy NER.\n",
    "    accepted_labels: list of spaCy entity labels to keep (e.g., ['PERSON','GPE','ORG','LOC']).\n",
    "    If None, keep most labels except DATE, TIME, PERCENT, MONEY, QUANTITY, ORDINAL, CARDINAL.\n",
    "    \"\"\"\n",
    "\n",
    "    if not text:\n",
    "        return []\n",
    "\n",
    "    doc = nlp(text)\n",
    "    entities = []\n",
    "    for ent in doc.ents:\n",
    "        if accepted_labels:\n",
    "            if ent.label_ in accepted_labels:\n",
    "                entities.append(ent.text)\n",
    "        else:\n",
    "            if ent.label_ not in {\"DATE\", \"TIME\", \"PERCENT\", \"MONEY\", \"QUANTITY\", \"ORDINAL\", \"CARDINAL\"}:\n",
    "                entities.append(ent.text)\n",
    "    return entities\n",
    "\n",
    "\n",
    "def clean_entity_token(token: str, stop_words_set, exclude_names_set):\n",
    "    t = token.strip()\n",
    "    # remove surrounding punctuation\n",
    "    t = re.sub(r\"^[\\W_]+|[\\W_]+$\", \"\", t)\n",
    "    # remove tokens that are pure digits or too short\n",
    "    if not t:\n",
    "        return None\n",
    "    if re.fullmatch(r\"\\d+\", t):\n",
    "        return None\n",
    "    if len(t) <= 1:\n",
    "        return None\n",
    "    low = t.lower()\n",
    "    if low in stop_words_set:\n",
    "        return None\n",
    "    # remove tokens that are exactly in exclude list\n",
    "    if low in exclude_names_set:\n",
    "        return None\n",
    "    # remove tokens that look like column headers\n",
    "    if re.match(r\"^identifier\\d*$\", low):\n",
    "        return None\n",
    "    return t\n",
    "\n",
    "\n",
    "# --------------------------- Wordcloud + plotting ---------------------------\n",
    "\n",
    "\n",
    "def save_wordcloud_from_freq(freq_dict, title, output_path, width=1600, height=800):\n",
    "    if not freq_dict:\n",
    "        logging.warning(f\"No data to generate wordcloud for {title}\")\n",
    "        return None\n",
    "\n",
    "    wc = WordCloud(width=width, height=height, background_color=\"white\", collocations=False)\n",
    "    wc.generate_from_frequencies(freq_dict)\n",
    "    plt.figure(figsize=(width / 200, height / 200))\n",
    "    plt.imshow(wc, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(title, fontsize=18)\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, dpi=150)\n",
    "    plt.close()\n",
    "    logging.info(f\"Saved wordcloud: {output_path}\")\n",
    "    return output_path\n",
    "\n",
    "\n",
    "# --------------------------- Main processing ---------------------------\n",
    "\n",
    "\n",
    "def process_file(\n",
    "    input_csv,\n",
    "    output_dir,\n",
    "    identifier_cols=None,\n",
    "    artifact_col=None,\n",
    "    exclude_names=None,\n",
    "    accepted_entity_labels=None,\n",
    "    top_k=500,\n",
    "):\n",
    "    setup_logging()\n",
    "    ensure_nltk_stopwords()\n",
    "\n",
    "    stop_words_set = set(stopwords.words(\"english\"))\n",
    "\n",
    "    # Default names to exclude (user requested) + common first names that may appear as noise.\n",
    "    default_exclude = {\n",
    "        \"anjali\",\n",
    "        \"shivani\",\n",
    "        \"shampy\",\n",
    "        \"sahili\",\n",
    "        \"shivangi\",\n",
    "        \"anjalisingh\",\n",
    "    }\n",
    "    if exclude_names:\n",
    "        exclude_names_set = set([n.strip().lower() for n in exclude_names]) | default_exclude\n",
    "    else:\n",
    "        exclude_names_set = default_exclude\n",
    "\n",
    "    logging.info(\"Loading spaCy model...\")\n",
    "    nlp = load_spacy_model()\n",
    "\n",
    "    logging.info(f\"Reading CSV file: {input_csv}\")\n",
    "    # try common encodings\n",
    "    try:\n",
    "        df = pd.read_csv(input_csv)\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Default read failed: {e}. Trying ISO-8859-1...\")\n",
    "        try:\n",
    "            df = pd.read_csv(input_csv, encoding=\"ISO-8859-1\")\n",
    "        except Exception as e2:\n",
    "            logging.error(f\"Failed to read CSV: {e2}\")\n",
    "            raise\n",
    "\n",
    "    original_columns = list(df.columns)\n",
    "    logging.info(f\"Columns detected: {original_columns}\")\n",
    "\n",
    "    # detect artifact and identifier columns if not provided\n",
    "    detected_artifact_col, detected_identifier_cols = detect_candidate_columns(df)\n",
    "    artifact_col = artifact_col or detected_artifact_col\n",
    "    if identifier_cols:\n",
    "        # verify provided columns exist\n",
    "        identifier_cols = [c for c in identifier_cols if c in df.columns]\n",
    "    else:\n",
    "        identifier_cols = detected_identifier_cols\n",
    "\n",
    "    logging.info(f\"Using artifact column: {artifact_col}\")\n",
    "    logging.info(f\"Using identifier columns: {identifier_cols}\")\n",
    "\n",
    "    # prepare output dir\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # --- Unique artifacts word frequencies ---\n",
    "    logging.info(\"Computing unique artifacts frequencies...\")\n",
    "    artifacts = df[artifact_col].dropna().astype(str).apply(sanitize_text)\n",
    "    artifact_freq = Counter()\n",
    "    artifact_unique_set = set()\n",
    "    for art in artifacts:\n",
    "        if not art:\n",
    "            continue\n",
    "        artifact_freq[art] += 1\n",
    "        artifact_unique_set.add(art)\n",
    "\n",
    "    artifact_token_freq = Counter()\n",
    "    artifact_whole_freq = Counter()\n",
    "    for art, cnt in artifact_freq.items():\n",
    "        artifact_whole_freq[art.replace(\" \", \"_\")] += cnt\n",
    "        for token in re.split(r\"[\\s,/|;-]+\", art):\n",
    "            t = token.strip()\n",
    "            tclean = clean_entity_token(t, stop_words_set, exclude_names_set)\n",
    "            if tclean:\n",
    "                artifact_token_freq[tclean] += cnt\n",
    "\n",
    "    # --- Identifier columns: collect NERs across those columns ---\n",
    "    logging.info(\"Extracting named entities from identifier columns...\")\n",
    "    all_ident_text = []\n",
    "    for col in identifier_cols:\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "        text_series = df[col].fillna(\"\").astype(str).apply(sanitize_text)\n",
    "        all_ident_text.extend(text_series.tolist())\n",
    "\n",
    "    # We'll concatenate texts into chunks to avoid overhead of calling spaCy on extremely long single string.\n",
    "    combined_texts = []\n",
    "    chunk_size = 200  # number of rows per spaCy pass; adjustable\n",
    "    for i in range(0, len(all_ident_text), chunk_size):\n",
    "        combined_texts.append(\" \".join(all_ident_text[i : i + chunk_size]))\n",
    "\n",
    "    entity_counter = Counter()\n",
    "    for chunk in tqdm(combined_texts, desc=\"spaCy NER chunks\"):\n",
    "        ents = extract_named_entities_from_text(nlp, chunk, accepted_labels=accepted_entity_labels)\n",
    "        for ent in ents:\n",
    "            # clean entity using same function, but keep multi-word entities\n",
    "            cleaned_phrase_tokens = []\n",
    "            for token in re.split(r\"[\\s,/|;:-]+\", ent):\n",
    "                cleaned = clean_entity_token(token, stop_words_set, exclude_names_set)\n",
    "                if cleaned:\n",
    "                    cleaned_phrase_tokens.append(cleaned)\n",
    "\n",
    "            # if phrase tokens exist, count both full phrase and tokens\n",
    "            if cleaned_phrase_tokens:\n",
    "                phrase = \" \".join(cleaned_phrase_tokens)\n",
    "                entity_counter[phrase] += 1\n",
    "                for t in cleaned_phrase_tokens:\n",
    "                    entity_counter[t] += 1\n",
    "\n",
    "    # Remove any residual column-names or extremely generic tokens from entity_counter\n",
    "    for bad in [\"identifier\", \"identifiers\"]:\n",
    "        if bad in entity_counter:\n",
    "            del entity_counter[bad]\n",
    "\n",
    "    # limit size\n",
    "    most_common_entities = dict(entity_counter.most_common(top_k))\n",
    "\n",
    "    # combined frequencies: artifact tokens + entities\n",
    "    combined_freq = Counter()\n",
    "    combined_freq.update(artifact_token_freq)\n",
    "    combined_freq.update(most_common_entities)\n",
    "\n",
    "    # ---------------- Save outputs ----------------\n",
    "    # Wordclouds\n",
    "    logging.info(\"Generating wordclouds and saving results...\")\n",
    "    artifacts_wordcloud_path = os.path.join(output_dir, \"artifacts_whole_wordcloud.png\")\n",
    "    save_wordcloud_from_freq(dict(artifact_whole_freq.most_common(1000)), \"Artifacts (whole names)\", artifacts_wordcloud_path)\n",
    "\n",
    "    artifact_token_wordcloud_path = os.path.join(output_dir, \"artifacts_tokens_wordcloud.png\")\n",
    "    save_wordcloud_from_freq(dict(artifact_token_freq.most_common(1000)), \"Artifacts (tokens)\", artifact_token_wordcloud_path)\n",
    "\n",
    "    entities_wordcloud_path = os.path.join(output_dir, \"identifiers_named_entities_wordcloud.png\")\n",
    "    save_wordcloud_from_freq(most_common_entities, \"Identifier Named Entities\", entities_wordcloud_path)\n",
    "\n",
    "    combined_wordcloud_path = os.path.join(output_dir, \"combined_artifacts_entities_wordcloud.png\")\n",
    "    save_wordcloud_from_freq(dict(combined_freq.most_common(1500)), \"Combined Artifacts + Entities\", combined_wordcloud_path)\n",
    "\n",
    "    # Save frequency CSVs\n",
    "    logging.info(\"Saving frequency CSVs...\")\n",
    "    pd.DataFrame(artifact_freq.most_common(), columns=[\"artifact_name\", \"count\"]).to_csv(\n",
    "        os.path.join(output_dir, \"artifact_freq.csv\"), index=False\n",
    "    )\n",
    "    pd.DataFrame(most_common_entities.items(), columns=[\"entity\", \"count\"]).to_csv(\n",
    "        os.path.join(output_dir, \"identifier_entity_freq.csv\"), index=False\n",
    "    )\n",
    "    pd.DataFrame(combined_freq.most_common(), columns=[\"token\", \"count\"]).to_csv(\n",
    "        os.path.join(output_dir, \"combined_token_freq.csv\"), index=False\n",
    "    )\n",
    "\n",
    "    logging.info(\"Processing complete. Outputs saved to: %s\", os.path.abspath(output_dir))\n",
    "    return {\n",
    "        \"artifact_whole_wordcloud\": artifacts_wordcloud_path,\n",
    "        \"artifact_token_wordcloud\": artifact_token_wordcloud_path,\n",
    "        \"entities_wordcloud\": entities_wordcloud_path,\n",
    "        \"combined_wordcloud\": combined_wordcloud_path,\n",
    "        \"artifact_freq_csv\": os.path.join(output_dir, \"artifact_freq.csv\"),\n",
    "        \"entity_freq_csv\": os.path.join(output_dir, \"identifier_entity_freq.csv\"),\n",
    "    }\n",
    "\n",
    "\n",
    "# --------------------------- CLI ---------------------------\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    p = argparse.ArgumentParser(description=\"Generate wordclouds from cultural artifact CSV.\")\n",
    "    # make optional here and validate later so script is notebook-friendly\n",
    "    p.add_argument(\"--input\", \"-i\", help=\"Input CSV file path\")\n",
    "    p.add_argument(\"--output_dir\", \"-o\", help=\"Output directory to save wordclouds and CSVs\")\n",
    "    p.add_argument(\n",
    "        \"--identifier_cols\",\n",
    "        \"-id\",\n",
    "        nargs=\"*\",\n",
    "        help=\"Identifier column names (space separated). If omitted, script auto-detects columns starting with 'Identifier'\",\n",
    "    )\n",
    "    p.add_argument(\n",
    "        \"--artifact_col\",\n",
    "        \"-a\",\n",
    "        help=\"Artifact column name. If omitted, script will attempt to auto-detect common artifact column names.\",\n",
    "    )\n",
    "    p.add_argument(\n",
    "        \"--exclude_names\",\n",
    "        \"-e\",\n",
    "        nargs=\"*\",\n",
    "        help=\"List of names to exclude from entity tokens (space separated).\",\n",
    "    )\n",
    "    p.add_argument(\n",
    "        \"--top_k\",\n",
    "        type=int,\n",
    "        default=500,\n",
    "        help=\"How many top entities to keep for generating wordclouds (default: 500)\",\n",
    "    )\n",
    "    return p.parse_args()\n",
    "\n",
    "\n",
    "def create_sample_csv(path):\n",
    "    logging.info(\"Creating a tiny sample CSV for demo purposes at: %s\", path)\n",
    "    sample = pd.DataFrame(\n",
    "        {\n",
    "            \"Unique Artifact\": [\n",
    "                \"Terracotta horse\",\n",
    "                \"Bronze bell\",\n",
    "                \"Stone sculpture of Vishnu\",\n",
    "                \"Handloom sari\",\n",
    "                \"Batik textile\",\n",
    "            ],\n",
    "            \"Identifier1\": [\n",
    "                \"Patna district, Bihar\",\n",
    "                \"Pataliputra museum\",\n",
    "                \"8th century temple\",\n",
    "                \"Weaver: Anjali Singh\",\n",
    "                \"Maker: Shivani Rao\",\n",
    "            ],\n",
    "            \"Identifier2\": [\n",
    "                \"Terracotta\",\n",
    "                \"Bronze\",\n",
    "                \"Stone\",\n",
    "                \"Textile\",\n",
    "                \"Textile\",\n",
    "            ],\n",
    "        }\n",
    "    )\n",
    "    sample.to_csv(path, index=False)\n",
    "    return path\n",
    "\n",
    "\n",
    "def main():\n",
    "    setup_logging()\n",
    "    args = parse_args()\n",
    "\n",
    "    # If user didn't pass CLI args and we're in an interactive environment, try to auto-fill sensible defaults\n",
    "    is_interactive = False\n",
    "    try:\n",
    "        # presence of get_ipython indicates notebook/ipython\n",
    "        is_interactive = \"get_ipython\" in globals()\n",
    "    except Exception:\n",
    "        is_interactive = False\n",
    "\n",
    "    input_csv = args.input\n",
    "    output_dir = args.output_dir or \"./wordcloud_output\"\n",
    "    identifier_cols = args.identifier_cols\n",
    "    artifact_col = args.artifact_col\n",
    "    exclude_names = args.exclude_names\n",
    "    top_k = args.top_k\n",
    "\n",
    "    # If no input provided, try to find a CSV in cwd (useful for notebook runs)\n",
    "    if not input_csv:\n",
    "        csvs = glob.glob(\"*.csv\")\n",
    "        if csvs:\n",
    "            input_csv = csvs[0]\n",
    "            logging.info(\"No --input provided. Auto-using first CSV in cwd: %s\", input_csv)\n",
    "        elif is_interactive:\n",
    "            # create a sample CSV in cwd for quick testing\n",
    "            sample_path = os.path.join(os.getcwd(), \"sample_cultural_artifacts_demo.csv\")\n",
    "            input_csv = create_sample_csv(sample_path)\n",
    "            logging.info(\"No CSV found in cwd; using generated sample CSV: %s\", input_csv)\n",
    "        else:\n",
    "            logging.error(\"No --input provided and not in interactive mode. Please provide --input and --output_dir.\")\n",
    "            print(\"Example CLI usage:\\n  python script.py --input /path/to/file.csv --output_dir /path/to/outdir\")\n",
    "            sys.exit(2)\n",
    "\n",
    "    if not output_dir:\n",
    "        # already set above, but ensure non-empty\n",
    "        output_dir = \"./wordcloud_output\"\n",
    "        logging.info(\"No --output_dir provided; defaulting to %s\", output_dir)\n",
    "\n",
    "    result = process_file(\n",
    "        input_csv=input_csv,\n",
    "        output_dir=output_dir,\n",
    "        identifier_cols=identifier_cols,\n",
    "        artifact_col=artifact_col,\n",
    "        exclude_names=exclude_names,\n",
    "        top_k=top_k,\n",
    "    )\n",
    "\n",
    "    logging.info(\"Finished. Result paths: %s\", result)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3aeedbdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--input INPUT] [--output_dir OUTPUT_DIR]\n",
      "                             [--identifier_cols [IDENTIFIER_COLS ...]]\n",
      "                             [--artifact_col ARTIFACT_COL]\n",
      "                             [--exclude_names [EXCLUDE_NAMES ...]]\n",
      "                             [--top_k TOP_K]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --f=/Users/anjalisingh/Library/Jupyter/runtime/kernel-v3c837de481c8781c0b020b95ef94903aa168f27d4.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 2\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Production-ready script to generate wordclouds from a cultural artifacts dataset.\n",
    "\n",
    "Usage (CLI):\n",
    "    python wordcloud_generator_for_cultural_artifacts.py \\\n",
    "        --input \"/path/to/cultural-artifact-gold-sheet-final.csv\" \\\n",
    "        --output_dir \"/path/to/output/dir\" \\\n",
    "        --identifier_cols Identifier1 Identifier2\n",
    "\n",
    "Or in a Jupyter notebook: just run the script; it will auto-detect a CSV in cwd or\n",
    "create a tiny sample CSV to demonstrate functionality.\n",
    "\n",
    "Requirements:\n",
    "    pip install pandas spacy wordcloud matplotlib nltk tqdm\n",
    "    python -m spacy download en_core_web_sm\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import logging\n",
    "import glob\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib\n",
    "# Use non-interactive backend for headless environments\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# spaCy and NLTK\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --------------------------- Helper utilities ---------------------------\n",
    "\n",
    "def setup_logging():\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "        handlers=[logging.StreamHandler(sys.stdout)],\n",
    "    )\n",
    "\n",
    "\n",
    "def ensure_nltk_stopwords():\n",
    "    try:\n",
    "        nltk.data.find(\"corpora/stopwords\")\n",
    "    except LookupError:\n",
    "        logging.info(\"Downloading NLTK stopwords...\")\n",
    "        nltk.download(\"stopwords\")\n",
    "\n",
    "\n",
    "def load_spacy_model(model_name=\"en_core_web_sm\"):\n",
    "    try:\n",
    "        nlp = spacy.load(model_name)\n",
    "    except OSError:\n",
    "        logging.info(f\"spaCy model {model_name} not found. Attempting to download...\")\n",
    "        from spacy.cli import download\n",
    "\n",
    "        download(model_name)\n",
    "        nlp = spacy.load(model_name)\n",
    "    return nlp\n",
    "\n",
    "\n",
    "def sanitize_text(text: str) -> str:\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    # remove newlines and weird whitespace\n",
    "    return re.sub(r\"\\s+\", \" \", str(text)).strip()\n",
    "\n",
    "\n",
    "def detect_candidate_columns(df: pd.DataFrame):\n",
    "    \"\"\"Detect artifact column and identifier columns if not explicitly provided.\"\"\"\n",
    "    col_lower = {c.lower(): c for c in df.columns}\n",
    "\n",
    "    artifact_candidates = [\n",
    "        \"unique artifact\",\n",
    "        \"cultural artifact\",\n",
    "        \"artifact\",\n",
    "        \"unique_artifact\",\n",
    "        \"cultural artifact\",\n",
    "    ]\n",
    "    artifact_col = None\n",
    "    for cand in artifact_candidates:\n",
    "        if cand in col_lower:\n",
    "            artifact_col = col_lower[cand]\n",
    "            break\n",
    "\n",
    "    # identifier columns: any column whose name starts with 'identifier' (case-insensitive)\n",
    "    identifier_cols = [c for c in df.columns if c.lower().startswith(\"identifier\")]\n",
    "\n",
    "    # fallback: look for numbered identifier columns\n",
    "    if not identifier_cols:\n",
    "        for i in range(1, 6):\n",
    "            name = f\"identifier{i}\"\n",
    "            if name in col_lower:\n",
    "                identifier_cols.append(col_lower[name])\n",
    "\n",
    "    # final fallback: pick columns likely to contain short descriptor text\n",
    "    if not artifact_col:\n",
    "        possible = [c for c in df.columns if \"artifact\" in c.lower() or \"unique\" in c.lower()]\n",
    "        artifact_col = possible[0] if possible else df.columns[0]\n",
    "\n",
    "    return artifact_col, identifier_cols\n",
    "\n",
    "\n",
    "# --------------------------- NER + cleaning ---------------------------\n",
    "\n",
    "def extract_named_entities_from_text(nlp, text, accepted_labels=None):\n",
    "    \"\"\"Return a list of extracted entity strings from text using spaCy NER.\n",
    "    accepted_labels: list of spaCy entity labels to keep (e.g., ['PERSON','GPE','ORG','LOC']).\n",
    "    If None, keep most labels except DATE, TIME, PERCENT, MONEY, QUANTITY, ORDINAL, CARDINAL.\n",
    "    \"\"\"\n",
    "\n",
    "    if not text:\n",
    "        return []\n",
    "\n",
    "    doc = nlp(text)\n",
    "    entities = []\n",
    "    for ent in doc.ents:\n",
    "        if accepted_labels:\n",
    "            if ent.label_ in accepted_labels:\n",
    "                entities.append(ent.text)\n",
    "        else:\n",
    "            if ent.label_ not in {\"DATE\", \"TIME\", \"PERCENT\", \"MONEY\", \"QUANTITY\", \"ORDINAL\", \"CARDINAL\"}:\n",
    "                entities.append(ent.text)\n",
    "    return entities\n",
    "\n",
    "\n",
    "def clean_entity_token(token: str, stop_words_set, exclude_names_set):\n",
    "    t = token.strip()\n",
    "    # remove surrounding punctuation\n",
    "    t = re.sub(r\"^[\\W_]+|[\\W_]+$\", \"\", t)\n",
    "    # remove tokens that are pure digits or too short\n",
    "    if not t:\n",
    "        return None\n",
    "    if re.fullmatch(r\"\\d+\", t):\n",
    "        return None\n",
    "    if len(t) <= 1:\n",
    "        return None\n",
    "    low = t.lower()\n",
    "    if low in stop_words_set:\n",
    "        return None\n",
    "    # remove tokens that are exactly in exclude list\n",
    "    if low in exclude_names_set:\n",
    "        return None\n",
    "    # remove tokens that look like column headers\n",
    "    if re.match(r\"^identifier\\d*$\", low):\n",
    "        return None\n",
    "    return t\n",
    "\n",
    "\n",
    "# --------------------------- Wordcloud + plotting ---------------------------\n",
    "\n",
    "\n",
    "def save_wordcloud_from_freq(freq_dict, title, output_path, width=1600, height=800):\n",
    "    if not freq_dict:\n",
    "        logging.warning(f\"No data to generate wordcloud for {title}\")\n",
    "        return None\n",
    "\n",
    "    wc = WordCloud(width=width, height=height, background_color=\"white\", collocations=False)\n",
    "    wc.generate_from_frequencies(freq_dict)\n",
    "    plt.figure(figsize=(width / 200, height / 200))\n",
    "    plt.imshow(wc, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(title, fontsize=18)\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, dpi=150)\n",
    "    plt.close()\n",
    "    logging.info(f\"Saved wordcloud: {output_path}\")\n",
    "    return output_path\n",
    "\n",
    "\n",
    "# --------------------------- Main processing ---------------------------\n",
    "\n",
    "\n",
    "def process_file(\n",
    "    input_csv,\n",
    "    output_dir,\n",
    "    identifier_cols=None,\n",
    "    artifact_col=None,\n",
    "    exclude_names=None,\n",
    "    accepted_entity_labels=None,\n",
    "    top_k=500,\n",
    "):\n",
    "    setup_logging()\n",
    "    ensure_nltk_stopwords()\n",
    "\n",
    "    stop_words_set = set(stopwords.words(\"english\"))\n",
    "\n",
    "    # Default names to exclude (user requested) + common first names that may appear as noise.\n",
    "    default_exclude = {\n",
    "        \"anjali\",\n",
    "        \"shivani\",\n",
    "        \"shampy\",\n",
    "        \"sahili\",\n",
    "        \"shivangi\",\n",
    "        \"anjalisingh\",\n",
    "    }\n",
    "    if exclude_names:\n",
    "        exclude_names_set = set([n.strip().lower() for n in exclude_names]) | default_exclude\n",
    "    else:\n",
    "        exclude_names_set = default_exclude\n",
    "\n",
    "    logging.info(\"Loading spaCy model...\")\n",
    "    nlp = load_spacy_model()\n",
    "\n",
    "    logging.info(f\"Reading CSV file: {input_csv}\")\n",
    "    # try common encodings\n",
    "    try:\n",
    "        df = pd.read_csv(input_csv)\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Default read failed: {e}. Trying ISO-8859-1...\")\n",
    "        try:\n",
    "            df = pd.read_csv(input_csv, encoding=\"ISO-8859-1\")\n",
    "        except Exception as e2:\n",
    "            logging.error(f\"Failed to read CSV: {e2}\")\n",
    "            raise\n",
    "\n",
    "    original_columns = list(df.columns)\n",
    "    logging.info(f\"Columns detected: {original_columns}\")\n",
    "\n",
    "    # detect artifact and identifier columns if not provided\n",
    "    detected_artifact_col, detected_identifier_cols = detect_candidate_columns(df)\n",
    "    artifact_col = artifact_col or detected_artifact_col\n",
    "    if identifier_cols:\n",
    "        # verify provided columns exist\n",
    "        identifier_cols = [c for c in identifier_cols if c in df.columns]\n",
    "    else:\n",
    "        identifier_cols = detected_identifier_cols\n",
    "\n",
    "    logging.info(f\"Using artifact column: {artifact_col}\")\n",
    "    logging.info(f\"Using identifier columns: {identifier_cols}\")\n",
    "\n",
    "    # prepare output dir\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # --- Unique artifacts word frequencies ---\n",
    "    logging.info(\"Computing unique artifacts frequencies...\")\n",
    "    artifacts = df[artifact_col].dropna().astype(str).apply(sanitize_text)\n",
    "    artifact_freq = Counter()\n",
    "    artifact_unique_set = set()\n",
    "    for art in artifacts:\n",
    "        if not art:\n",
    "            continue\n",
    "        artifact_freq[art] += 1\n",
    "        artifact_unique_set.add(art)\n",
    "\n",
    "    artifact_token_freq = Counter()\n",
    "    artifact_whole_freq = Counter()\n",
    "    for art, cnt in artifact_freq.items():\n",
    "        artifact_whole_freq[art.replace(\" \", \"_\")] += cnt\n",
    "        for token in re.split(r\"[\\s,/|;-]+\", art):\n",
    "            t = token.strip()\n",
    "            tclean = clean_entity_token(t, stop_words_set, exclude_names_set)\n",
    "            if tclean:\n",
    "                artifact_token_freq[tclean] += cnt\n",
    "\n",
    "    # --- Identifier columns: collect NERs across those columns ---\n",
    "    logging.info(\"Extracting named entities from identifier columns...\")\n",
    "    all_ident_text = []\n",
    "    for col in identifier_cols:\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "        text_series = df[col].fillna(\"\").astype(str).apply(sanitize_text)\n",
    "        all_ident_text.extend(text_series.tolist())\n",
    "\n",
    "    # We'll concatenate texts into chunks to avoid overhead of calling spaCy on extremely long single string.\n",
    "    combined_texts = []\n",
    "    chunk_size = 200  # number of rows per spaCy pass; adjustable\n",
    "    for i in range(0, len(all_ident_text), chunk_size):\n",
    "        combined_texts.append(\" \".join(all_ident_text[i : i + chunk_size]))\n",
    "\n",
    "    entity_counter = Counter()\n",
    "    for chunk in tqdm(combined_texts, desc=\"spaCy NER chunks\"):\n",
    "        ents = extract_named_entities_from_text(nlp, chunk, accepted_labels=accepted_entity_labels)\n",
    "        for ent in ents:\n",
    "            # clean entity using same function, but keep multi-word entities\n",
    "            cleaned_phrase_tokens = []\n",
    "            for token in re.split(r\"[\\s,/|;:-]+\", ent):\n",
    "                cleaned = clean_entity_token(token, stop_words_set, exclude_names_set)\n",
    "                if cleaned:\n",
    "                    cleaned_phrase_tokens.append(cleaned)\n",
    "\n",
    "            # if phrase tokens exist, count both full phrase and tokens\n",
    "            if cleaned_phrase_tokens:\n",
    "                phrase = \" \".join(cleaned_phrase_tokens)\n",
    "                entity_counter[phrase] += 1\n",
    "                for t in cleaned_phrase_tokens:\n",
    "                    entity_counter[t] += 1\n",
    "\n",
    "    # Remove any residual column-names or extremely generic tokens from entity_counter\n",
    "    for bad in [\"identifier\", \"identifiers\"]:\n",
    "        if bad in entity_counter:\n",
    "            del entity_counter[bad]\n",
    "\n",
    "    # limit size\n",
    "    most_common_entities = dict(entity_counter.most_common(top_k))\n",
    "\n",
    "    # combined frequencies: artifact tokens + entities\n",
    "    combined_freq = Counter()\n",
    "    combined_freq.update(artifact_token_freq)\n",
    "    combined_freq.update(most_common_entities)\n",
    "\n",
    "    # ---------------- Save outputs ----------------\n",
    "    # Wordclouds\n",
    "    logging.info(\"Generating wordclouds and saving results...\")\n",
    "    artifacts_wordcloud_path = os.path.join(output_dir, \"artifacts_whole_wordcloud.png\")\n",
    "    save_wordcloud_from_freq(dict(artifact_whole_freq.most_common(1000)), \"Artifacts (whole names)\", artifacts_wordcloud_path)\n",
    "\n",
    "    artifact_token_wordcloud_path = os.path.join(output_dir, \"artifacts_tokens_wordcloud.png\")\n",
    "    save_wordcloud_from_freq(dict(artifact_token_freq.most_common(1000)), \"Artifacts (tokens)\", artifact_token_wordcloud_path)\n",
    "\n",
    "    entities_wordcloud_path = os.path.join(output_dir, \"identifiers_named_entities_wordcloud.png\")\n",
    "    save_wordcloud_from_freq(most_common_entities, \"Identifier Named Entities\", entities_wordcloud_path)\n",
    "\n",
    "    combined_wordcloud_path = os.path.join(output_dir, \"combined_artifacts_entities_wordcloud.png\")\n",
    "    save_wordcloud_from_freq(dict(combined_freq.most_common(1500)), \"Combined Artifacts + Entities\", combined_wordcloud_path)\n",
    "\n",
    "    # Save frequency CSVs\n",
    "    logging.info(\"Saving frequency CSVs...\")\n",
    "    pd.DataFrame(artifact_freq.most_common(), columns=[\"artifact_name\", \"count\"]).to_csv(\n",
    "        os.path.join(output_dir, \"artifact_freq.csv\"), index=False\n",
    "    )\n",
    "    pd.DataFrame(most_common_entities.items(), columns=[\"entity\", \"count\"]).to_csv(\n",
    "        os.path.join(output_dir, \"identifier_entity_freq.csv\"), index=False\n",
    "    )\n",
    "    pd.DataFrame(combined_freq.most_common(), columns=[\"token\", \"count\"]).to_csv(\n",
    "        os.path.join(output_dir, \"combined_token_freq.csv\"), index=False\n",
    "    )\n",
    "\n",
    "    logging.info(\"Processing complete. Outputs saved to: %s\", os.path.abspath(output_dir))\n",
    "    return {\n",
    "        \"artifact_whole_wordcloud\": artifacts_wordcloud_path,\n",
    "        \"artifact_token_wordcloud\": artifact_token_wordcloud_path,\n",
    "        \"entities_wordcloud\": entities_wordcloud_path,\n",
    "        \"combined_wordcloud\": combined_wordcloud_path,\n",
    "        \"artifact_freq_csv\": os.path.join(output_dir, \"artifact_freq.csv\"),\n",
    "        \"entity_freq_csv\": os.path.join(output_dir, \"identifier_entity_freq.csv\"),\n",
    "    }\n",
    "\n",
    "\n",
    "# --------------------------- CLI ---------------------------\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    p = argparse.ArgumentParser(description=\"Generate wordclouds from cultural artifact CSV.\")\n",
    "    # make optional here and validate later so script is notebook-friendly\n",
    "    p.add_argument(\"--input\", \"-i\", help=\"Input CSV file path\")\n",
    "    p.add_argument(\"--output_dir\", \"-o\", help=\"Output directory to save wordclouds and CSVs\")\n",
    "    p.add_argument(\n",
    "        \"--identifier_cols\",\n",
    "        \"-id\",\n",
    "        nargs=\"*\",\n",
    "        help=\"Identifier column names (space separated). If omitted, script auto-detects columns starting with 'Identifier'\",\n",
    "    )\n",
    "    p.add_argument(\n",
    "        \"--artifact_col\",\n",
    "        \"-a\",\n",
    "        help=\"Artifact column name. If omitted, script will attempt to auto-detect common artifact column names.\",\n",
    "    )\n",
    "    p.add_argument(\n",
    "        \"--exclude_names\",\n",
    "        \"-e\",\n",
    "        nargs=\"*\",\n",
    "        help=\"List of names to exclude from entity tokens (space separated).\",\n",
    "    )\n",
    "    p.add_argument(\n",
    "        \"--top_k\",\n",
    "        type=int,\n",
    "        default=500,\n",
    "        help=\"How many top entities to keep for generating wordclouds (default: 500)\",\n",
    "    )\n",
    "    return p.parse_args()\n",
    "\n",
    "\n",
    "def create_sample_csv(path):\n",
    "    logging.info(\"Creating a tiny sample CSV for demo purposes at: %s\", path)\n",
    "    sample = pd.DataFrame(\n",
    "        {\n",
    "            \"Unique Artifact\": [\n",
    "                \"Terracotta horse\",\n",
    "                \"Bronze bell\",\n",
    "                \"Stone sculpture of Vishnu\",\n",
    "                \"Handloom sari\",\n",
    "                \"Batik textile\",\n",
    "            ],\n",
    "            \"Identifier1\": [\n",
    "                \"Patna district, Bihar\",\n",
    "                \"Pataliputra museum\",\n",
    "                \"8th century temple\",\n",
    "                \"Weaver: Anjali Singh\",\n",
    "                \"Maker: Shivani Rao\",\n",
    "            ],\n",
    "            \"Identifier2\": [\n",
    "                \"Terracotta\",\n",
    "                \"Bronze\",\n",
    "                \"Stone\",\n",
    "                \"Textile\",\n",
    "                \"Textile\",\n",
    "            ],\n",
    "        }\n",
    "    )\n",
    "    sample.to_csv(path, index=False)\n",
    "    return path\n",
    "\n",
    "\n",
    "def main():\n",
    "    setup_logging()\n",
    "    args = parse_args()\n",
    "\n",
    "    # If user didn't pass CLI args and we're in an interactive environment, try to auto-fill sensible defaults\n",
    "    is_interactive = False\n",
    "    try:\n",
    "        # presence of get_ipython indicates notebook/ipython\n",
    "        is_interactive = \"get_ipython\" in globals()\n",
    "    except Exception:\n",
    "        is_interactive = False\n",
    "\n",
    "    input_csv = args.input\n",
    "    output_dir = args.output_dir or \"./wordcloud_output\"\n",
    "    identifier_cols = args.identifier_cols\n",
    "    artifact_col = args.artifact_col\n",
    "    exclude_names = args.exclude_names\n",
    "    top_k = args.top_k\n",
    "\n",
    "    # If no input provided, try to find a CSV in cwd (useful for notebook runs)\n",
    "    if not input_csv:\n",
    "        csvs = glob.glob(\"*.csv\")\n",
    "        if csvs:\n",
    "            input_csv = csvs[0]\n",
    "            logging.info(\"No --input provided. Auto-using first CSV in cwd: %s\", input_csv)\n",
    "        elif is_interactive:\n",
    "            # create a sample CSV in cwd for quick testing\n",
    "            sample_path = os.path.join(os.getcwd(), \"sample_cultural_artifacts_demo.csv\")\n",
    "            input_csv = create_sample_csv(sample_path)\n",
    "            logging.info(\"No CSV found in cwd; using generated sample CSV: %s\", input_csv)\n",
    "        else:\n",
    "            logging.error(\"No --input provided and not in interactive mode. Please provide --input and --output_dir.\")\n",
    "            print(\"Example CLI usage:\\n  python script.py --input /path/to/file.csv --output_dir /path/to/outdir\")\n",
    "            sys.exit(2)\n",
    "\n",
    "    if not output_dir:\n",
    "        # already set above, but ensure non-empty\n",
    "        output_dir = \"./wordcloud_output\"\n",
    "        logging.info(\"No --output_dir provided; defaulting to %s\", output_dir)\n",
    "\n",
    "    result = process_file(\n",
    "        input_csv=input_csv,\n",
    "        output_dir=output_dir,\n",
    "        identifier_cols=identifier_cols,\n",
    "        artifact_col=artifact_col,\n",
    "        exclude_names=exclude_names,\n",
    "        top_k=top_k,\n",
    "    )\n",
    "\n",
    "    logging.info(\"Finished. Result paths: %s\", result)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af45ad4a",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expected 'except' or 'finally' block (3596940498.py, line 196)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 196\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mexce\u001b[39m\n    ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m expected 'except' or 'finally' block\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Production-ready script to generate wordclouds from a cultural artifacts dataset.\n",
    "\n",
    "Usage (CLI):\n",
    "    python wordcloud_generator_for_cultural_artifacts.py \\\n",
    "        --input \"/path/to/cultural-artifact-gold-sheet-final.csv\" \\\n",
    "        --output_dir \"/path/to/output/dir\" \\\n",
    "        --identifier_cols Identifier1 Identifier2\n",
    "\n",
    "In Jupyter/interactive sessions the script will ignore kernel-injected args and:\n",
    " - auto-use the first CSV in cwd if present, OR\n",
    " - create a small sample CSV and run on it for demo.\n",
    "\n",
    "Requirements:\n",
    "    pip install pandas spacy wordcloud matplotlib nltk tqdm\n",
    "    python -m spacy download en_core_web_sm\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import logging\n",
    "import glob\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib\n",
    "# use non-interactive backend so script works headless\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --------------------------- Helper utilities ---------------------------\n",
    "\n",
    "def setup_logging():\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "        handlers=[logging.StreamHandler(sys.stdout)],\n",
    "    )\n",
    "\n",
    "def ensure_nltk_stopwords():\n",
    "    try:\n",
    "        nltk.data.find(\"corpora/stopwords\")\n",
    "    except LookupError:\n",
    "        logging.info(\"Downloading NLTK stopwords...\")\n",
    "        nltk.download(\"stopwords\")\n",
    "\n",
    "def load_spacy_model(model_name=\"en_core_web_sm\"):\n",
    "    try:\n",
    "        nlp = spacy.load(model_name)\n",
    "    except OSError:\n",
    "        logging.info(f\"spaCy model {model_name} not found. Attempting to download...\")\n",
    "        from spacy.cli import download\n",
    "        download(model_name)\n",
    "        nlp = spacy.load(model_name)\n",
    "    return nlp\n",
    "\n",
    "def sanitize_text(text: str) -> str:\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    return re.sub(r\"\\s+\", \" \", str(text)).strip()\n",
    "\n",
    "def detect_candidate_columns(df: pd.DataFrame):\n",
    "    \"\"\"Detect artifact column and identifier columns if not explicitly provided.\"\"\"\n",
    "    col_lower = {c.lower(): c for c in df.columns}\n",
    "\n",
    "    artifact_candidates = [\n",
    "        \"unique artifact\",\n",
    "        \"cultural artifact\",\n",
    "        \"artifact\",\n",
    "        \"unique_artifact\",\n",
    "        \"cultural artifact\",\n",
    "    ]\n",
    "    artifact_col = None\n",
    "    for cand in artifact_candidates:\n",
    "        if cand in col_lower:\n",
    "            artifact_col = col_lower[cand]\n",
    "            break\n",
    "\n",
    "    # identifier columns: any column whose name starts with 'identifier' (case-insensitive)\n",
    "    identifier_cols = [c for c in df.columns if c.lower().startswith(\"identifier\")]\n",
    "\n",
    "    # fallback: look for numbered identifier columns\n",
    "    if not identifier_cols:\n",
    "        for i in range(1, 6):\n",
    "            name = f\"identifier{i}\"\n",
    "            if name in col_lower:\n",
    "                identifier_cols.append(col_lower[name])\n",
    "\n",
    "    # final fallback: pick columns likely to contain short descriptor text\n",
    "    if not artifact_col:\n",
    "        possible = [c for c in df.columns if \"artifact\" in c.lower() or \"unique\" in c.lower()]\n",
    "        artifact_col = possible[0] if possible else df.columns[0]\n",
    "\n",
    "    return artifact_col, identifier_cols\n",
    "\n",
    "# --------------------------- NER + cleaning ---------------------------\n",
    "\n",
    "def extract_named_entities_from_text(nlp, text, accepted_labels=None):\n",
    "    if not text:\n",
    "        return []\n",
    "    doc = nlp(text)\n",
    "    entities = []\n",
    "    for ent in doc.ents:\n",
    "        if accepted_labels:\n",
    "            if ent.label_ in accepted_labels:\n",
    "                entities.append(ent.text)\n",
    "        else:\n",
    "            if ent.label_ not in {\"DATE\", \"TIME\", \"PERCENT\", \"MONEY\", \"QUANTITY\", \"ORDINAL\", \"CARDINAL\"}:\n",
    "                entities.append(ent.text)\n",
    "    return entities\n",
    "\n",
    "def clean_entity_token(token: str, stop_words_set, exclude_names_set):\n",
    "    t = token.strip()\n",
    "    t = re.sub(r\"^[\\W_]+|[\\W_]+$\", \"\", t)\n",
    "    if not t:\n",
    "        return None\n",
    "    if re.fullmatch(r\"\\d+\", t):\n",
    "        return None\n",
    "    if len(t) <= 1:\n",
    "        return None\n",
    "    low = t.lower()\n",
    "    if low in stop_words_set:\n",
    "        return None\n",
    "    if low in exclude_names_set:\n",
    "        return None\n",
    "    if re.match(r\"^identifier\\d*$\", low):\n",
    "        return None\n",
    "    return t\n",
    "\n",
    "# --------------------------- Wordcloud + plotting ---------------------------\n",
    "\n",
    "def save_wordcloud_from_freq(freq_dict, title, output_path, width=1600, height=800):\n",
    "    if not freq_dict:\n",
    "        logging.warning(f\"No data to generate wordcloud for {title}\")\n",
    "        return None\n",
    "\n",
    "    wc = WordCloud(width=width, height=height, background_color=\"white\", collocations=False)\n",
    "    wc.generate_from_frequencies(freq_dict)\n",
    "    plt.figure(figsize=(width / 200, height / 200))\n",
    "    plt.imshow(wc, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(title, fontsize=18)\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, dpi=150)\n",
    "    plt.close()\n",
    "    logging.info(f\"Saved wordcloud: {output_path}\")\n",
    "    return output_path\n",
    "\n",
    "# --------------------------- Main processing ---------------------------\n",
    "\n",
    "def process_file(\n",
    "    input_csv,\n",
    "    output_dir,\n",
    "    identifier_cols=None,\n",
    "    artifact_col=None,\n",
    "    exclude_names=None,\n",
    "    accepted_entity_labels=None,\n",
    "    top_k=500,\n",
    "):\n",
    "    setup_logging()\n",
    "    ensure_nltk_stopwords()\n",
    "\n",
    "    stop_words_set = set(stopwords.words(\"english\"))\n",
    "\n",
    "    # Default exclude names (add more if needed)\n",
    "    default_exclude = {\n",
    "        \"anjali\",\n",
    "        \"shivani\",\n",
    "        \"shampy\",\n",
    "        \"sahili\",\n",
    "        \"shivangi\",\n",
    "        \"anjalisingh\",\n",
    "    }\n",
    "    exclude_names_set = (set(n.strip().lower() for n in exclude_names) | default_exclude) if exclude_names else default_exclude\n",
    "\n",
    "    logging.info(\"Loading spaCy model...\")\n",
    "    nlp = load_spacy_model()\n",
    "\n",
    "    logging.info(f\"Reading CSV file: {input_csv}\")\n",
    "    try:\n",
    "        df = pd.read_csv(input_csv)\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Default read failed: {e}. Trying ISO-8859-1...\")\n",
    "        try:\n",
    "            df = pd.read_csv(input_csv, encoding=\"ISO-8859-1\")\n",
    "        exce\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b440026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-06 01:19:44,875 - INFO - Ignoring unknown CLI args: ['--f=/Users/anjalisingh/Library/Jupyter/runtime/kernel-v3c837de481c8781c0b020b95ef94903aa168f27d4.json']\n",
      "2025-10-06 01:19:44,877 - INFO - No --input provided. Auto-using first CSV in cwd: Full Dataset - cultural-artifact-gold-sheet-final.csv\n",
      "2025-10-06 01:19:44,882 - INFO - Loading spaCy model...\n",
      "2025-10-06 01:19:45,414 - INFO - Reading CSV file: Full Dataset - cultural-artifact-gold-sheet-final.csv\n",
      "2025-10-06 01:19:45,457 - INFO - Columns detected: ['Attribute', 'unique artifact', 'Specific Location', 'state', 'Identifier1', 'Identifier2', 'Identifier3', 'Identifier4', 'Old Identifier', 'Influence Locations', 'Image Link1', 'Image Link2', 'Unnamed: 12', 'Unnamed: 13', '{Identifier} famous throughout India, originated in {specific location}', 'Unnamed: 15', 'Unnamed: 16', 'Unnamed: 17', 'Unnamed: 18', 'Unnamed: 19', 'Unnamed: 20', 'Unnamed: 21', 'Unnamed: 22', 'Unnamed: 23', 'Unnamed: 24', 'Attribute.1', 'state.1', 'Unnamed: 27']\n",
      "2025-10-06 01:19:45,457 - INFO - Using artifact column: unique artifact\n",
      "2025-10-06 01:19:45,458 - INFO - Using identifier columns: ['Identifier1', 'Identifier2', 'Identifier3', 'Identifier4']\n",
      "2025-10-06 01:19:45,459 - INFO - Computing unique artifacts frequencies...\n",
      "2025-10-06 01:19:45,473 - INFO - Extracting named entities from identifier columns...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spaCy NER chunks: 100%|██████████| 19/19 [00:08<00:00,  2.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-06 01:19:53,919 - INFO - Generating wordclouds and saving results...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-06 01:19:56,148 - INFO - Saved wordcloud: ./wordcloud_output/artifacts_whole_wordcloud.png\n",
      "2025-10-06 01:19:58,137 - INFO - Saved wordcloud: ./wordcloud_output/artifacts_tokens_wordcloud.png\n",
      "2025-10-06 01:20:00,107 - INFO - Saved wordcloud: ./wordcloud_output/identifiers_named_entities_wordcloud.png\n",
      "2025-10-06 01:20:02,126 - INFO - Saved wordcloud: ./wordcloud_output/combined_artifacts_entities_wordcloud.png\n",
      "2025-10-06 01:20:02,126 - INFO - Saving frequency CSVs...\n",
      "2025-10-06 01:20:02,135 - INFO - Processing complete. Outputs saved to: /Users/anjalisingh/Desktop/IITP/WordCloud/wordcloud_output\n",
      "2025-10-06 01:20:02,139 - INFO - Finished. Result paths: {'artifact_whole_wordcloud': './wordcloud_output/artifacts_whole_wordcloud.png', 'artifact_token_wordcloud': './wordcloud_output/artifacts_tokens_wordcloud.png', 'entities_wordcloud': './wordcloud_output/identifiers_named_entities_wordcloud.png', 'combined_wordcloud': './wordcloud_output/combined_artifacts_entities_wordcloud.png', 'artifact_freq_csv': './wordcloud_output/artifact_freq.csv', 'entity_freq_csv': './wordcloud_output/identifier_entity_freq.csv'}\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Production-ready script to generate wordclouds from a cultural artifacts dataset.\n",
    "\n",
    "Usage (CLI):\n",
    "    python wordcloud_generator_for_cultural_artifacts.py \\\n",
    "        --input \"/path/to/cultural-artifact-gold-sheet-final.csv\" \\\n",
    "        --output_dir \"/path/to/output/dir\" \\\n",
    "        --identifier_cols Identifier1 Identifier2\n",
    "\n",
    "In Jupyter/interactive sessions the script will ignore kernel-injected args and:\n",
    " - auto-use the first CSV in cwd if present, OR\n",
    " - create a small sample CSV and run on it for demo.\n",
    "\n",
    "Requirements:\n",
    "    pip install pandas spacy wordcloud matplotlib nltk tqdm\n",
    "    python -m spacy download en_core_web_sm\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import logging\n",
    "import glob\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib\n",
    "# use non-interactive backend so script works headless\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --------------------------- Helper utilities ---------------------------\n",
    "\n",
    "def setup_logging():\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "        handlers=[logging.StreamHandler(sys.stdout)],\n",
    "    )\n",
    "\n",
    "def ensure_nltk_stopwords():\n",
    "    try:\n",
    "        nltk.data.find(\"corpora/stopwords\")\n",
    "    except LookupError:\n",
    "        logging.info(\"Downloading NLTK stopwords...\")\n",
    "        nltk.download(\"stopwords\")\n",
    "\n",
    "def load_spacy_model(model_name=\"en_core_web_sm\"):\n",
    "    try:\n",
    "        nlp = spacy.load(model_name)\n",
    "    except OSError:\n",
    "        logging.info(f\"spaCy model {model_name} not found. Attempting to download...\")\n",
    "        from spacy.cli import download\n",
    "        download(model_name)\n",
    "        nlp = spacy.load(model_name)\n",
    "    return nlp\n",
    "\n",
    "def sanitize_text(text: str) -> str:\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    return re.sub(r\"\\s+\", \" \", str(text)).strip()\n",
    "\n",
    "def detect_candidate_columns(df: pd.DataFrame):\n",
    "    \"\"\"Detect artifact column and identifier columns if not explicitly provided.\"\"\"\n",
    "    col_lower = {c.lower(): c for c in df.columns}\n",
    "\n",
    "    artifact_candidates = [\n",
    "        \"unique artifact\",\n",
    "        \"cultural artifact\",\n",
    "        \"artifact\",\n",
    "        \"unique_artifact\",\n",
    "        \"cultural artifact\",\n",
    "    ]\n",
    "    artifact_col = None\n",
    "    for cand in artifact_candidates:\n",
    "        if cand in col_lower:\n",
    "            artifact_col = col_lower[cand]\n",
    "            break\n",
    "\n",
    "    # identifier columns: any column whose name starts with 'identifier' (case-insensitive)\n",
    "    identifier_cols = [c for c in df.columns if c.lower().startswith(\"identifier\")]\n",
    "\n",
    "    # fallback: look for numbered identifier columns\n",
    "    if not identifier_cols:\n",
    "        for i in range(1, 6):\n",
    "            name = f\"identifier{i}\"\n",
    "            if name in col_lower:\n",
    "                identifier_cols.append(col_lower[name])\n",
    "\n",
    "    # final fallback: pick columns likely to contain short descriptor text\n",
    "    if not artifact_col:\n",
    "        possible = [c for c in df.columns if \"artifact\" in c.lower() or \"unique\" in c.lower()]\n",
    "        artifact_col = possible[0] if possible else df.columns[0]\n",
    "\n",
    "    return artifact_col, identifier_cols\n",
    "\n",
    "# --------------------------- NER + cleaning ---------------------------\n",
    "\n",
    "def extract_named_entities_from_text(nlp, text, accepted_labels=None):\n",
    "    if not text:\n",
    "        return []\n",
    "    doc = nlp(text)\n",
    "    entities = []\n",
    "    for ent in doc.ents:\n",
    "        if accepted_labels:\n",
    "            if ent.label_ in accepted_labels:\n",
    "                entities.append(ent.text)\n",
    "        else:\n",
    "            if ent.label_ not in {\"DATE\", \"TIME\", \"PERCENT\", \"MONEY\", \"QUANTITY\", \"ORDINAL\", \"CARDINAL\"}:\n",
    "                entities.append(ent.text)\n",
    "    return entities\n",
    "\n",
    "def clean_entity_token(token: str, stop_words_set, exclude_names_set):\n",
    "    t = token.strip()\n",
    "    t = re.sub(r\"^[\\W_]+|[\\W_]+$\", \"\", t)\n",
    "    if not t:\n",
    "        return None\n",
    "    if re.fullmatch(r\"\\d+\", t):\n",
    "        return None\n",
    "    if len(t) <= 1:\n",
    "        return None\n",
    "    low = t.lower()\n",
    "    if low in stop_words_set:\n",
    "        return None\n",
    "    if low in exclude_names_set:\n",
    "        return None\n",
    "    if re.match(r\"^identifier\\d*$\", low):\n",
    "        return None\n",
    "    return t\n",
    "\n",
    "# --------------------------- Wordcloud + plotting ---------------------------\n",
    "\n",
    "def save_wordcloud_from_freq(freq_dict, title, output_path, width=1600, height=800):\n",
    "    if not freq_dict:\n",
    "        logging.warning(f\"No data to generate wordcloud for {title}\")\n",
    "        return None\n",
    "\n",
    "    wc = WordCloud(width=width, height=height, background_color=\"white\", collocations=False)\n",
    "    wc.generate_from_frequencies(freq_dict)\n",
    "    plt.figure(figsize=(width / 200, height / 200))\n",
    "    plt.imshow(wc, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(title, fontsize=18)\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, dpi=150)\n",
    "    plt.close()\n",
    "    logging.info(f\"Saved wordcloud: {output_path}\")\n",
    "    return output_path\n",
    "\n",
    "# --------------------------- Main processing ---------------------------\n",
    "\n",
    "def process_file(\n",
    "    input_csv,\n",
    "    output_dir,\n",
    "    identifier_cols=None,\n",
    "    artifact_col=None,\n",
    "    exclude_names=None,\n",
    "    accepted_entity_labels=None,\n",
    "    top_k=500,\n",
    "):\n",
    "    setup_logging()\n",
    "    ensure_nltk_stopwords()\n",
    "\n",
    "    stop_words_set = set(stopwords.words(\"english\"))\n",
    "\n",
    "    # Default exclude names (add more if needed)\n",
    "    default_exclude = {\n",
    "        \"anjali\",\n",
    "        \"shivani\",\n",
    "        \"shampy\",\n",
    "        \"sahili\",\n",
    "        \"shivangi\",\n",
    "        \"anjalisingh\",\n",
    "    }\n",
    "    exclude_names_set = (set(n.strip().lower() for n in exclude_names) | default_exclude) if exclude_names else default_exclude\n",
    "\n",
    "    logging.info(\"Loading spaCy model...\")\n",
    "    nlp = load_spacy_model()\n",
    "\n",
    "    logging.info(f\"Reading CSV file: {input_csv}\")\n",
    "    try:\n",
    "        df = pd.read_csv(input_csv)\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Default read failed: {e}. Trying ISO-8859-1...\")\n",
    "        try:\n",
    "            df = pd.read_csv(input_csv, encoding=\"ISO-8859-1\")\n",
    "        except Exception as e2:\n",
    "            logging.error(f\"Failed to read CSV: {e2}\")\n",
    "            raise\n",
    "\n",
    "    original_columns = list(df.columns)\n",
    "    logging.info(f\"Columns detected: {original_columns}\")\n",
    "\n",
    "    detected_artifact_col, detected_identifier_cols = detect_candidate_columns(df)\n",
    "    artifact_col = artifact_col or detected_artifact_col\n",
    "    if identifier_cols:\n",
    "        identifier_cols = [c for c in identifier_cols if c in df.columns]\n",
    "    else:\n",
    "        identifier_cols = detected_identifier_cols\n",
    "\n",
    "    logging.info(f\"Using artifact column: {artifact_col}\")\n",
    "    logging.info(f\"Using identifier columns: {identifier_cols}\")\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # --- Unique artifacts word frequencies ---\n",
    "    logging.info(\"Computing unique artifacts frequencies...\")\n",
    "    artifacts = df[artifact_col].dropna().astype(str).apply(sanitize_text)\n",
    "    artifact_freq = Counter()\n",
    "    artifact_unique_set = set()\n",
    "    for art in artifacts:\n",
    "        if not art:\n",
    "            continue\n",
    "        artifact_freq[art] += 1\n",
    "        artifact_unique_set.add(art)\n",
    "\n",
    "    artifact_token_freq = Counter()\n",
    "    artifact_whole_freq = Counter()\n",
    "    for art, cnt in artifact_freq.items():\n",
    "        artifact_whole_freq[art.replace(\" \", \"_\")] += cnt\n",
    "        for token in re.split(r\"[\\s,/|;-]+\", art):\n",
    "            t = token.strip()\n",
    "            tclean = clean_entity_token(t, stop_words_set, exclude_names_set)\n",
    "            if tclean:\n",
    "                artifact_token_freq[tclean] += cnt\n",
    "\n",
    "    # --- Identifier columns: collect NERs across those columns ---\n",
    "    logging.info(\"Extracting named entities from identifier columns...\")\n",
    "    all_ident_text = []\n",
    "    for col in identifier_cols:\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "        text_series = df[col].fillna(\"\").astype(str).apply(sanitize_text)\n",
    "        all_ident_text.extend(text_series.tolist())\n",
    "\n",
    "    combined_texts = []\n",
    "    chunk_size = 200\n",
    "    for i in range(0, len(all_ident_text), chunk_size):\n",
    "        combined_texts.append(\" \".join(all_ident_text[i : i + chunk_size]))\n",
    "\n",
    "    entity_counter = Counter()\n",
    "    for chunk in tqdm(combined_texts, desc=\"spaCy NER chunks\"):\n",
    "        ents = extract_named_entities_from_text(nlp, chunk, accepted_labels=accepted_entity_labels)\n",
    "        for ent in ents:\n",
    "            cleaned_phrase_tokens = []\n",
    "            for token in re.split(r\"[\\s,/|;:-]+\", ent):\n",
    "                cleaned = clean_entity_token(token, stop_words_set, exclude_names_set)\n",
    "                if cleaned:\n",
    "                    cleaned_phrase_tokens.append(cleaned)\n",
    "            if cleaned_phrase_tokens:\n",
    "                phrase = \" \".join(cleaned_phrase_tokens)\n",
    "                entity_counter[phrase] += 1\n",
    "                for t in cleaned_phrase_tokens:\n",
    "                    entity_counter[t] += 1\n",
    "\n",
    "    for bad in [\"identifier\", \"identifiers\"]:\n",
    "        if bad in entity_counter:\n",
    "            del entity_counter[bad]\n",
    "\n",
    "    most_common_entities = dict(entity_counter.most_common(top_k))\n",
    "\n",
    "    combined_freq = Counter()\n",
    "    combined_freq.update(artifact_token_freq)\n",
    "    combined_freq.update(most_common_entities)\n",
    "\n",
    "    # ---------------- Save outputs ----------------\n",
    "    logging.info(\"Generating wordclouds and saving results...\")\n",
    "    artifacts_wordcloud_path = os.path.join(output_dir, \"artifacts_whole_wordcloud.png\")\n",
    "    save_wordcloud_from_freq(dict(artifact_whole_freq.most_common(1000)), \"Artifacts (whole names)\", artifacts_wordcloud_path)\n",
    "\n",
    "    artifact_token_wordcloud_path = os.path.join(output_dir, \"artifacts_tokens_wordcloud.png\")\n",
    "    save_wordcloud_from_freq(dict(artifact_token_freq.most_common(1000)), \"Artifacts (tokens)\", artifact_token_wordcloud_path)\n",
    "\n",
    "    entities_wordcloud_path = os.path.join(output_dir, \"identifiers_named_entities_wordcloud.png\")\n",
    "    save_wordcloud_from_freq(most_common_entities, \"Identifier Named Entities\", entities_wordcloud_path)\n",
    "\n",
    "    combined_wordcloud_path = os.path.join(output_dir, \"combined_artifacts_entities_wordcloud.png\")\n",
    "    save_wordcloud_from_freq(dict(combined_freq.most_common(1500)), \"Combined Artifacts + Entities\", combined_wordcloud_path)\n",
    "\n",
    "    logging.info(\"Saving frequency CSVs...\")\n",
    "    pd.DataFrame(artifact_freq.most_common(), columns=[\"artifact_name\", \"count\"]).to_csv(\n",
    "        os.path.join(output_dir, \"artifact_freq.csv\"), index=False\n",
    "    )\n",
    "    pd.DataFrame(most_common_entities.items(), columns=[\"entity\", \"count\"]).to_csv(\n",
    "        os.path.join(output_dir, \"identifier_entity_freq.csv\"), index=False\n",
    "    )\n",
    "    pd.DataFrame(combined_freq.most_common(), columns=[\"token\", \"count\"]).to_csv(\n",
    "        os.path.join(output_dir, \"combined_token_freq.csv\"), index=False\n",
    "    )\n",
    "\n",
    "    logging.info(\"Processing complete. Outputs saved to: %s\", os.path.abspath(output_dir))\n",
    "    return {\n",
    "        \"artifact_whole_wordcloud\": artifacts_wordcloud_path,\n",
    "        \"artifact_token_wordcloud\": artifact_token_wordcloud_path,\n",
    "        \"entities_wordcloud\": entities_wordcloud_path,\n",
    "        \"combined_wordcloud\": combined_wordcloud_path,\n",
    "        \"artifact_freq_csv\": os.path.join(output_dir, \"artifact_freq.csv\"),\n",
    "        \"entity_freq_csv\": os.path.join(output_dir, \"identifier_entity_freq.csv\"),\n",
    "    }\n",
    "\n",
    "# --------------------------- CLI ---------------------------\n",
    "\n",
    "def parse_args():\n",
    "    p = argparse.ArgumentParser(description=\"Generate wordclouds from cultural artifact CSV.\")\n",
    "    p.add_argument(\"--input\", \"-i\", help=\"Input CSV file path\")\n",
    "    p.add_argument(\"--output_dir\", \"-o\", help=\"Output directory to save wordclouds and CSVs\")\n",
    "    p.add_argument(\n",
    "        \"--identifier_cols\",\n",
    "        \"-id\",\n",
    "        nargs=\"*\",\n",
    "        help=\"Identifier column names (space separated). If omitted, script auto-detects columns starting with 'Identifier'\",\n",
    "    )\n",
    "    p.add_argument(\n",
    "        \"--artifact_col\",\n",
    "        \"-a\",\n",
    "        help=\"Artifact column name. If omitted, script will attempt to auto-detect common artifact column names.\",\n",
    "    )\n",
    "    p.add_argument(\n",
    "        \"--exclude_names\",\n",
    "        \"-e\",\n",
    "        nargs=\"*\",\n",
    "        help=\"List of names to exclude from entity tokens (space separated).\",\n",
    "    )\n",
    "    p.add_argument(\n",
    "        \"--top_k\",\n",
    "        type=int,\n",
    "        default=500,\n",
    "        help=\"How many top entities to keep for generating wordclouds (default: 500)\",\n",
    "    )\n",
    "    # Use parse_known_args to avoid crash from extra kernel args in interactive environments\n",
    "    args, unknown = p.parse_known_args()\n",
    "    if unknown:\n",
    "        logging.getLogger().info(f\"Ignoring unknown CLI args: {unknown}\")\n",
    "    return args\n",
    "\n",
    "def create_sample_csv(path):\n",
    "    logging.info(\"Creating a tiny sample CSV for demo purposes at: %s\", path)\n",
    "    sample = pd.DataFrame(\n",
    "        {\n",
    "            \"Unique Artifact\": [\n",
    "                \"Terracotta horse\",\n",
    "                \"Bronze bell\",\n",
    "                \"Stone sculpture of Vishnu\",\n",
    "                \"Handloom sari\",\n",
    "                \"Batik textile\",\n",
    "            ],\n",
    "            \"Identifier1\": [\n",
    "                \"Patna district, Bihar\",\n",
    "                \"Pataliputra museum\",\n",
    "                \"8th century temple\",\n",
    "                \"Weaver: Anjali Singh\",\n",
    "                \"Maker: Shivani Rao\",\n",
    "            ],\n",
    "            \"Identifier2\": [\n",
    "                \"Terracotta\",\n",
    "                \"Bronze\",\n",
    "                \"Stone\",\n",
    "                \"Textile\",\n",
    "                \"Textile\",\n",
    "            ],\n",
    "        }\n",
    "    )\n",
    "    sample.to_csv(path, index=False)\n",
    "    return path\n",
    "\n",
    "def main():\n",
    "    setup_logging()\n",
    "    args = parse_args()\n",
    "\n",
    "    # detect interactive environment (Jupyter/IPython)\n",
    "    is_interactive = False\n",
    "    try:\n",
    "        is_interactive = \"get_ipython\" in globals()\n",
    "    except Exception:\n",
    "        is_interactive = False\n",
    "\n",
    "    input_csv = args.input\n",
    "    output_dir = args.output_dir or \"./wordcloud_output\"\n",
    "    identifier_cols = args.identifier_cols\n",
    "    artifact_col = args.artifact_col\n",
    "    exclude_names = args.exclude_names\n",
    "    top_k = args.top_k\n",
    "\n",
    "    # If no input provided, try to find a CSV in cwd (useful for notebook runs)\n",
    "    if not input_csv:\n",
    "        csvs = glob.glob(\"*.csv\")\n",
    "        if csvs:\n",
    "            input_csv = csvs[0]\n",
    "            logging.info(\"No --input provided. Auto-using first CSV in cwd: %s\", input_csv)\n",
    "        elif is_interactive:\n",
    "            sample_path = os.path.join(os.getcwd(), \"sample_cultural_artifacts_demo.csv\")\n",
    "            input_csv = create_sample_csv(sample_path)\n",
    "            logging.info(\"No CSV found in cwd; using generated sample CSV: %s\", input_csv)\n",
    "        else:\n",
    "            logging.error(\"No --input provided and not in interactive mode. Please provide --input and --output_dir.\")\n",
    "            print(\"Example CLI usage:\\n  python script.py --input /path/to/file.csv --output_dir /path/to/outdir\")\n",
    "            sys.exit(2)\n",
    "\n",
    "    if not output_dir:\n",
    "        output_dir = \"./wordcloud_output\"\n",
    "        logging.info(\"No --output_dir provided; defaulting to %s\", output_dir)\n",
    "\n",
    "    # Run processing\n",
    "    result = process_file(\n",
    "        input_csv=input_csv,\n",
    "        output_dir=output_dir,\n",
    "        identifier_cols=identifier_cols,\n",
    "        artifact_col=artifact_col,\n",
    "        exclude_names=exclude_names,\n",
    "        top_k=top_k,\n",
    "    )\n",
    "\n",
    "    logging.info(\"Finished. Result paths: %s\", result)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

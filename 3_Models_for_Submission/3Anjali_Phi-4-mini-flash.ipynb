{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba992c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Phi-4-mini batch inference with optional token\n",
    "# -------------------------------\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pandas as pd\n",
    "import gc\n",
    "import time\n",
    "import os\n",
    "\n",
    "# -------------------------------\n",
    "# 0. Memory cleanup\n",
    "# -------------------------------\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Load dataset\n",
    "# -------------------------------\n",
    "try:\n",
    "    final_datasheet = \"finalDataset.csv\"\n",
    "    data = pd.read_csv(final_datasheet, encoding=\"utf-8\")\n",
    "except UnicodeDecodeError:\n",
    "    print(\"UTF-8 failed, trying ISO-8859-1...\")\n",
    "    data = pd.read_csv(final_datasheet, encoding=\"ISO-8859-1\")\n",
    "\n",
    "data['ModelAnswer'] = \"\"\n",
    "data['Correct'] = \"\"\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Model name & optional token\n",
    "# -------------------------------\n",
    "model_name = \"microsoft/Phi-4-mini-flash-reasoning\"\n",
    "huggingface_token = os.getenv(\"HF_TOKEN\")  # optional\n",
    "\n",
    "if huggingface_token is None:\n",
    "    print(\"⚠️ No Hugging Face token found. Will try to download public model without token.\")\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Load tokenizer and model\n",
    "# -------------------------------\n",
    "token_kwargs = {\"use_auth_token\": huggingface_token} if huggingface_token else {}\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, **token_kwargs)\n",
    "\n",
    "print(\"Loading model...\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    **token_kwargs,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Generate function\n",
    "# -------------------------------\n",
    "def generate_answer(prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=100,\n",
    "            temperature=0.0,\n",
    "            do_sample=False\n",
    "        )\n",
    "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "    return decoded.replace(prompt.strip(), \"\").strip()\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Loop through dataset\n",
    "# -------------------------------\n",
    "start_time = time.time()\n",
    "total_questions = len(data)\n",
    "correct_answers = 0\n",
    "\n",
    "for idx, row in data.iterrows():\n",
    "    prompt = f\"\"\"\n",
    "You are a person from India with deep knowledge and lived experience of Indian culture.\n",
    "Now, answer the following question using your expertise in Indian culture by identifying the specific cultural element being referred to.\n",
    "Respond only with the name of the cultural element (e.g., Indian) — no additional text, questions, or explanations.\n",
    "\n",
    "Question: {row['Corrected Question']}\n",
    "\"\"\"\n",
    "    model_answer = generate_answer(prompt)\n",
    "    prediction_correctness = row['Answer'].strip().lower() in model_answer.lower()\n",
    "    if prediction_correctness:\n",
    "        correct_answers += 1\n",
    "\n",
    "    data.at[idx, 'ModelAnswer'] = model_answer\n",
    "    data.at[idx, 'Correct'] = str(prediction_correctness)\n",
    "\n",
    "    print(f\"Progress: {idx+1}/{total_questions} | Correct so far: {correct_answers}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 6. Summary & save\n",
    "# -------------------------------\n",
    "accuracy = (correct_answers / total_questions) * 100\n",
    "print(f\"Total correct: {correct_answers}/{total_questions}\")\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "output_file = \"model_answers_results_phi_mini.csv\"\n",
    "data.to_csv(output_file, index=False)\n",
    "print(f\"Results saved to {output_file}\")\n",
    "print(f\"Total runtime: {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "# -------------------------------\n",
    "# 7. Cleanup\n",
    "# -------------------------------\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

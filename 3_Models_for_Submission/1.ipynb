{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7996f19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dfe08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows:\n",
      "                                                     predicted_desirability  \\\n",
      "0  Which dish, famous for made with apricots and ...                    YES   \n",
      "1  Which tourist attraction, famous for built dur...                    YES   \n",
      "2  Which temple, famous for hindu temple situated...                     NO   \n",
      "3  Which temple, famous for hindu temple of godde...                    YES   \n",
      "4  Which costume, famous for most notable for the...                    YES   \n",
      "\n",
      "   probability_yes                                         graph_path  \\\n",
      "0         0.992769  ['personalities:dia_mirza', 'state:telangana',...   \n",
      "1         0.991918  ['history:khursheed_jah_devdi', 'state:telanga...   \n",
      "2         0.764582  ['history:fox_sagar_lake', 'state:telangana', ...   \n",
      "3         0.997859  ['tourism:lumbini_park', 'state:telangana', 'h...   \n",
      "4         0.984486  ['tourism:makkah_masjid_hyderabad', 'state:tel...   \n",
      "\n",
      "     template type  Manual Score (1-5) Manual Desirability (YES/NO)  \\\n",
      "0  generic_v6    A                 4.0                          YES   \n",
      "1  generic_v6    A                 3.5                          YES   \n",
      "2  generic_v6    A                 3.0                           NO   \n",
      "3  generic_v6    A                 4.0                          YES   \n",
      "4  generic_v6    A                 3.0                          YES   \n",
      "\n",
      "                                             COMMENT  \n",
      "0                                                NaN  \n",
      "1                                                NaN  \n",
      "2  river is a continious entity and cannot be cal...  \n",
      "3                                                NaN  \n",
      "4                                                NaN  \n",
      "\n",
      "Column names:\n",
      "Index([' ', 'predicted_desirability', 'probability_yes', 'graph_path',\n",
      "       'template', 'type', 'Manual Score (1-5)',\n",
      "       'Manual Desirability (YES/NO)', 'COMMENT'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sheet_url = \"https://docs.google.com/spreadsheets/d/1MEm2m6NZ_xVd5eOE38ebFoXytq8klIGlBz25gXfALrs/export?format=csv\"\n",
    "\n",
    "# Try reading\n",
    "data = pd.read_csv(sheet_url)\n",
    "\n",
    "# Load model\n",
    "model_name = \"gpt2\"  # Replace with your trained model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "# Step 3: Check first 5 rows and columns\n",
    "print(\"First 5 rows:\")\n",
    "print(data.head())\n",
    "\n",
    "print(\"\\nColumn names:\")\n",
    "print(data.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "93e67818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to /tmp/my_datasheet.csv\n"
     ]
    }
   ],
   "source": [
    "data.to_csv(\"/tmp/my_datasheet.csv\", index=False)\n",
    "print(\"Saved to /tmp/my_datasheet.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b62c94db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2: Load Model\n",
    "# -----------------------------\n",
    "model_name = \"gpt2\"  # Replace with your trained model if needed\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fc599d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# Step 3: Inference Loop\n",
    "# -----------------------------\n",
    "model_answers = []\n",
    "predicted_correctly = []\n",
    "correct_answers = 0\n",
    "total_questions = len(data)\n",
    "progress = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "810f1f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "question_with_options = \"You are a person from India having deep knowledge of Indian culture. Now answer the following question: What is the capital of India? Options: A) Delhi B) Mumbai C) Kolkata D) Chennai\"\n",
    "\n",
    " # Tokenize\n",
    "inputs = tokenizer.encode(question_with_options, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Generate output\n",
    "with torch.no_grad():\n",
    "        outputs = model.generate(inputs, max_length=150)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee20fe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No question found'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5a11c203",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'question'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/core/indexes/base.py:3805\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3804\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3805\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3806\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'question'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[51]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m index, row \u001b[38;5;129;01min\u001b[39;00m data.iterrows():\n\u001b[32m      2\u001b[39m     question_with_options = (\n\u001b[32m      3\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mYou are a person from India having deep knowledge and lived experience of Indian culture. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNow answer the following question: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mquestion\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      5\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mProvide the full answer as given in the dataset.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      6\u001b[39m     )\n\u001b[32m      8\u001b[39m     answer_text = tokenizer.decode(outputs[\u001b[32m0\u001b[39m], skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      9\u001b[39m     \u001b[38;5;28mprint\u001b[39m(answer_text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/core/series.py:1121\u001b[39m, in \u001b[36mSeries.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   1118\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._values[key]\n\u001b[32m   1120\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[32m-> \u001b[39m\u001b[32m1121\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1123\u001b[39m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[32m   1124\u001b[39m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[32m   1125\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/core/series.py:1237\u001b[39m, in \u001b[36mSeries._get_value\u001b[39m\u001b[34m(self, label, takeable)\u001b[39m\n\u001b[32m   1234\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._values[label]\n\u001b[32m   1236\u001b[39m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1237\u001b[39m loc = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1239\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[32m   1240\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._values[loc]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/core/indexes/base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3807\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3808\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3809\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3810\u001b[39m     ):\n\u001b[32m   3811\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3814\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3815\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3816\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3817\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'question'"
     ]
    }
   ],
   "source": [
    "for index, row in data.iterrows():\n",
    "    question_with_options = (\n",
    "        f\"You are a person from India having deep knowledge and lived experience of Indian culture. \"\n",
    "        f\"Now answer the following question: {row['question']} \"\n",
    "        f\"Provide the full answer as given in the dataset.\"\n",
    "    )\n",
    "    \n",
    "    answer_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(answer_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d894f93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "   # Decode\n",
    "model_answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "model_answer = model_answer.replace(question_with_options, \"\").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1636c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # Check correctness\n",
    "if row['Answer'].strip().lower() in model_answer.strip().lower():\n",
    "        prediction_correctness = 'True'\n",
    "        correct_answers += 1\n",
    "else:\n",
    "        prediction_correctness = 'False'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925935c1",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Append results\n",
    "model_answers.append(model_answer)\n",
    "predicted_correctly.append(prediction_correctness)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733e80eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Progress\n",
    "progress += 1\n",
    "print(f\"Progress: {progress}/{total_questions} | Model: {model_answer} | Correct: {row['Answer']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f33f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Step 4: Save Results\n",
    "# -----------------------------\n",
    "data['Model_Answer'] = model_answers\n",
    "data['Prediction_Correct'] = predicted_correctly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1c7ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to /tmp since Jupyter might be read-only\n",
    "output_path = \"/tmp/inference_results.csv\"\n",
    "data.to_csv(output_path, index=False)\n",
    "print(f\"\\nResults saved to {output_path}\")\n",
    "print(f\"Total correct answers: {correct_answers}/{total_questions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d559ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "model_name = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "data = pd.read_csv(\"Full Dataset - Final Dataset.csv\")\n",
    "data\n",
    "model_answers = []\n",
    "predicted_correctly = []\n",
    "correct_answers = 0\n",
    "progress = 0\n",
    "total_questions = len(data)\n",
    "for index, row in data.iterrows():\n",
    "    \n",
    "    question_with_options = (\n",
    "        f\"You are a person from India having deep knowledge and lived experience of Indian culture. \"\n",
    "        f\"Now answer the following question by selecting the correct option from the options listed below. \"\n",
    "        f\"Respond with the full answer choice (e.g., 'C) Indian') and nothing else. \"\n",
    "        f\"Do not include additional questions, explanations, or text.\"\n",
    "        f\"\\nQuestion: {row['question']} \"\n",
    "        f\"\\nOptions: \"\n",
    "        f\"A) {row['answer']} \"\n",
    "        f\"B) {row['answer']} \"\n",
    "        f\"C) {row['answer']} \"\n",
    "        f\"D) {row['answer']}\"\n",
    "    )\n",
    " \n",
    "print(data.columns)\n",
    "messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": question_with_options}\n",
    "    ]\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# Generate answer\n",
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=150\n",
    ")\n",
    "\n",
    " # Keep only generated tokens (exclude input)\n",
    "generated_ids = [\n",
    "output_ids[len(input_ids):]\n",
    "for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "\n",
    "# Decode text\n",
    "model_answer = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "\n",
    "# Check correctness\n",
    "if str(row['answer']).strip().lower() in model_answer.strip().lower():\n",
    "    prediction_correctness = 'True'\n",
    "    correct_answers += 1\n",
    "else:\n",
    "    prediction_correctness = 'False'\n",
    "\n",
    " # Progress update\n",
    "progress += 1\n",
    "print(f\"\\nProgress: {progress}/{total_questions}\")\n",
    "print(f\"Model Answer: {model_answer}\")\n",
    "print(f\"Correct Answer: {row['answer']}\")\n",
    "print(\"Correct answers so far:\", correct_answers)\n",
    "\n",
    " # Save results\n",
    "model_answers.append(model_answer)\n",
    "predicted_correctly.append(prediction_correctness)\n",
    "print(len(data), len(model_answers), len(predicted_correctly))\n",
    "model_answers = []\n",
    "predicted_correctly = []\n",
    "\n",
    "# loop through your dataframe rows\n",
    "for index, row in data.iterrows():\n",
    "    # run inference\n",
    "    model_answer = \"...\"  # your model’s answer\n",
    "    is_correct = True     # or however you compute correctness\n",
    "\n",
    "    # store results\n",
    "    model_answers.append(model_answer)\n",
    "    predicted_correctly.append(is_correct)\n",
    "\n",
    "# now lengths will match\n",
    "print(len(data), len(model_answers), len(predicted_correctly))  # should all be 3284\n",
    "\n",
    "# Save results\n",
    "data[\"model_answers\"] = model_answers\n",
    "data[\"Correctness\"] = predicted_correctly\n",
    "\n",
    "output_path = \"Qwen2.5_Inference_Results.csv\"\n",
    "data.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"\\nInference completed. Results saved to {output_path}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935731b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf5b18c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cd608e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f4289c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import time\n",
    "import torch\n",
    "import gc\n",
    "import os\n",
    "\n",
    "try:\n",
    "    data = pd.read_csv(r\"finalDataset.csv\", encoding='ISO-8859-1')\n",
    "except UnicodeDecodeError:\n",
    "    print(\"Failed to load with ISO-8859-1 encoding, trying windows-1252\")\n",
    "    data = pd.read_csv(r\"finalDataset.csv\", encoding='windows-1252')\n",
    "\n",
    "gc.collect()  # Run garbage collection to free CPU RAM\n",
    "torch.cuda.empty_cache()  # Clear GPU memory\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "start_time = time.time()\n",
    "model_name = \"Telugu-LLM-Labs/Indic-gemma-7b-finetuned-sft-Navarasa-2.0\"  # Replace with the actual model name if different\n",
    "huggingface_token = 'HUGGINGFACE_TOKEN = \"YOUR_TOKEN_HERE\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=huggingface_token)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, token=huggingface_token)\n",
    "model.to(device)\n",
    "print(\"Model and tokenizer loaded successfully\")\n",
    "\n",
    "progress = 0\n",
    "correct_answers = 0\n",
    "total_questions = len(data)\n",
    "model_answers = []\n",
    "selected_options = []\n",
    "predicted_correctly = []\n",
    "for index, row in data.iterrows():\n",
    "    question_with_options = f\"\"\"\n",
    "    You are a person from India with deep knowledge and lived experience of Indian culture.\n",
    "    Now, answer the following question using your expertise in Indian culture by identifying the specific cultural element being referred to.\n",
    "    Respond only with the name of the cultural element (e.g., Indian) â€” no additional text, questions, or explanations.\n",
    "\n",
    "    Question: {row['Corrected Question']}\n",
    "\"\"\"\n",
    "    \n",
    "    inputs = tokenizer.encode(question_with_options, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(inputs, max_length=1000)\n",
    "    model_answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    model_answer = model_answer.replace(question_with_options, \"\").strip()\n",
    "\n",
    "    if row['answer'].strip().lower() in model_answer.strip().lower():\n",
    "        prediction_correctness = 'True'\n",
    "        correct_answers += 1\n",
    "    else:\n",
    "        prediction_correctness = 'False'\n",
    "\n",
    "    progress += 1\n",
    "    print(f\"Progress: {progress}/{total_questions}\")\n",
    "    print(f\"Model Answer: {model_answer}\")\n",
    "    print(f\"Correct Answer: {row['Answer']}\")\n",
    "    model_answers.append(model_answer)\n",
    "    predicted_correctly.append(prediction_correctness)\n",
    "    print(\"Correct answers so far:\", correct_answers)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = correct_answers / total_questions\n",
    "print(f\"Final Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(f\"Total Correct Answers: {correct_answers}\")\n",
    "data['Model_Answer'] = model_answers\n",
    "data['Predicted Correctly'] = predicted_correctly\n",
    "data.to_csv('finalDataset_teluguLLM.csv', index=False)\n",
    "end_time = time.time()\n",
    "time_taken = end_time - start_time\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

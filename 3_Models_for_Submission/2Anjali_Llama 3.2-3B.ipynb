{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fda960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Llama 3.2-3B batch inference (safe for GPU) + save results\n",
    "# -------------------------------\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pandas as pd\n",
    "import gc\n",
    "import time\n",
    "import os\n",
    "\n",
    "# -------------------------------\n",
    "# 0. Memory cleanup before start\n",
    "# -------------------------------\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Load dataset (with encoding fallback)\n",
    "# -------------------------------\n",
    "try:\n",
    "    final_datasheet = \"finalDataset.csv\"\n",
    "    data = pd.read_csv(final_datasheet, encoding=\"utf-8\")\n",
    "except UnicodeDecodeError:\n",
    "    print(\"UTF-8 failed, trying ISO-8859-1...\")\n",
    "    data = pd.read_csv(final_datasheet, encoding=\"ISO-8859-1\")\n",
    "\n",
    "# Add empty columns for storing results\n",
    "data['ModelAnswer'] = \"\"\n",
    "data['Correct'] = \"\"\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Load model and tokenizer\n",
    "# -------------------------------\n",
    "model_name = \"meta-llama/Llama-3.2-3b\"  # <- updated model\n",
    "\n",
    "# Load token from environment variable (set by whoever runs it)\n",
    "huggingface_token = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "# Check token presence\n",
    "if huggingface_token is None:\n",
    "    print(\"⚠️ No Hugging Face token found. Please set HF_TOKEN environment variable.\")\n",
    "\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=huggingface_token)\n",
    "\n",
    "print(\"Loading model on GPU...\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    token=huggingface_token,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Batch inference function\n",
    "# -------------------------------\n",
    "def generate_answer(prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=100,\n",
    "            temperature=0.0,\n",
    "            do_sample=False\n",
    "        )\n",
    "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "    # Clean: remove question echo if present\n",
    "    return decoded.replace(prompt.strip(), \"\").strip()\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Loop through dataset\n",
    "# -------------------------------\n",
    "start_time = time.time()\n",
    "\n",
    "total_questions = len(data)\n",
    "correct_answers = 0\n",
    "\n",
    "for idx, row in data.iterrows():\n",
    "    prompt = f\"\"\"\n",
    "You are a person from India with deep knowledge and lived experience of Indian culture.\n",
    "Now, answer the following question using your expertise in Indian culture by identifying the specific cultural element being referred to.\n",
    "Respond only with the name of the cultural element (e.g., Indian) — no additional text, questions, or explanations.\n",
    "\n",
    "Question: {row['Corrected Question']}\n",
    "\"\"\"\n",
    "    model_answer = generate_answer(prompt)\n",
    "\n",
    "    # check correctness\n",
    "    prediction_correctness = row['Answer'].strip().lower() in model_answer.lower()\n",
    "    if prediction_correctness:\n",
    "        correct_answers += 1\n",
    "\n",
    "    data.at[idx, 'ModelAnswer'] = model_answer\n",
    "    data.at[idx, 'Correct'] = str(prediction_correctness)  # store as string True/False\n",
    "\n",
    "    print(f\"Progress: {idx+1}/{total_questions} | Correct so far: {correct_answers}\")\n",
    "    print(f\"Model Answer: {model_answer}\")\n",
    "    print(f\"Correct Answer: {row['Answer']}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Summary\n",
    "# -------------------------------\n",
    "accuracy = (correct_answers / total_questions) * 100\n",
    "print(f\"Total correct: {correct_answers}/{total_questions}\")\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# -------------------------------\n",
    "# 6. Save results\n",
    "# -------------------------------\n",
    "output_file = \"model_answers_results_3b.csv\"\n",
    "data.to_csv(output_file, index=False)\n",
    "print(f\"Results saved to {output_file}\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Total runtime: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# -------------------------------\n",
    "# 7. Cleanup after run\n",
    "# -------------------------------\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

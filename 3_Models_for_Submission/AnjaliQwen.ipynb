{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0449ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Complete runnable script:\n",
    "- Loads finalDataset.csv\n",
    "- Generates a \"Corrected Question\" style rewrite using Qwen2.5-1.5B-Instruct\n",
    "- Saves incremental results to `corrected_questions_qwen.csv`\n",
    "- Resumes if output file already exists\n",
    "Notes:\n",
    "- If you have a GPU, it will use it. If not, it runs on CPU (slower).\n",
    "- Install required packages if missing: transformers, torch, pandas\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import time\n",
    "import traceback\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "# -----------------------\n",
    "# Config\n",
    "# -----------------------\n",
    "FINAL_DATASHEET = \"finalDataset.csv\"                     # input file (your file)\n",
    "OUTPUT_FILE = \"corrected_questions_qwen.csv\"             # output file\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-1.5B-Instruct\"                # small Qwen model (works locally)\n",
    "SAVE_EVERY = 20                                          # save after this many rows\n",
    "MAX_NEW_TOKENS = 150\n",
    "DO_SAMPLE = False\n",
    "TEMPERATURE = 0.0\n",
    "# -----------------------\n",
    "\n",
    "# 0. Basic checks\n",
    "if not os.path.exists(FINAL_DATASHEET):\n",
    "    raise FileNotFoundError(f\"Input file not found: {FINAL_DATASHEET}\")\n",
    "\n",
    "# 1. Load dataset\n",
    "data = pd.read_csv(FINAL_DATASHEET)\n",
    "print(\"Loaded dataset:\", FINAL_DATASHEET, \"shape =\", data.shape)\n",
    "print(\"Columns:\", list(data.columns))\n",
    "\n",
    "# Determine which input column to use (prefer 'Corrected Question' if you want original corrected text,\n",
    "# otherwise use the raw 'question' column to rewrite into a corrected question)\n",
    "INPUT_COL = \"Corrected Question\" if \"Corrected Question\" in data.columns else \"question\"\n",
    "print(\"Using input column for generation:\", INPUT_COL)\n",
    "\n",
    "# Prepare output columns if not present\n",
    "if \"Qwen_Corrected_Question\" not in data.columns:\n",
    "    data[\"Qwen_Corrected_Question\"] = \"\"\n",
    "\n",
    "if \"Qwen_Generation_Error\" not in data.columns:\n",
    "    data[\"Qwen_Generation_Error\"] = \"\"\n",
    "\n",
    "# If output exists already (resume), load it and skip processed rows\n",
    "if os.path.exists(OUTPUT_FILE):\n",
    "    print(\"Found existing output file. Loading to resume...\")\n",
    "    existing = pd.read_csv(OUTPUT_FILE)\n",
    "    # if columns match, adopt existing results to resume\n",
    "    if \"Qwen_Corrected_Question\" in existing.columns:\n",
    "        # overwrite the data rows we have results for\n",
    "        existing_idx = existing.index\n",
    "        data.loc[existing_idx, \"Qwen_Corrected_Question\"] = existing[\"Qwen_Corrected_Question\"]\n",
    "        if \"Qwen_Generation_Error\" in existing.columns:\n",
    "            data.loc[existing_idx, \"Qwen_Generation_Error\"] = existing[\"Qwen_Generation_Error\"]\n",
    "        print(\"Resumed from existing results. Rows already processed:\", existing[\"Qwen_Corrected_Question\"].notna().sum())\n",
    "\n",
    "# 2. Device detection\n",
    "has_cuda = torch.cuda.is_available()\n",
    "device_str = \"cuda\" if has_cuda else \"cpu\"\n",
    "print(\"Device:\", device_str, \"| CUDA available:\", has_cuda)\n",
    "\n",
    "# 3. Load model & tokenizer\n",
    "print(f\"Loading model {MODEL_NAME} ... (this may take a bit)\")\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "    # choose dtype\n",
    "    torch_dtype = torch.float16 if has_cuda else None\n",
    "\n",
    "    # If GPU available, let transformers auto map; else load to CPU\n",
    "    if has_cuda:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch_dtype,\n",
    "            low_cpu_mem_usage=True,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "        # create pipeline with device 0 (first GPU)\n",
    "        text_gen = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=0, trust_remote_code=True)\n",
    "    else:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            device_map=\"cpu\",\n",
    "            torch_dtype=torch_dtype,\n",
    "            low_cpu_mem_usage=True,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "        text_gen = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=-1, trust_remote_code=True)\n",
    "\n",
    "    print(\"Model & tokenizer loaded.\")\n",
    "except Exception as e:\n",
    "    print(\"Failed to load model/tokenizer. Error:\")\n",
    "    traceback.print_exc()\n",
    "    raise e\n",
    "\n",
    "# 4. Helper: build prompt and generate\n",
    "def build_prompt_from_row(row):\n",
    "    # Use available columns safely (some rows may have NaN)\n",
    "    q = str(row.get(\"question\", \"\") or \"\")\n",
    "    a = str(row.get(\"answer\", \"\") or \"\")\n",
    "    sf = str(row.get(\"supporting_facts\", \"\") or \"\")\n",
    "    gp = str(row.get(\"graph_path\", \"\") or \"\")\n",
    "    tp = str(row.get(\"type\", \"\") or \"\")\n",
    "    src = str(row.get(\"source\", \"\") or \"\")\n",
    "    prob = str(row.get(\"prob_yes\", \"\") or \"\")\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are given the following information from a dataset. Use the details to produce a clear, concise, self-contained \"Corrected Question\" that can be answered independently.\n",
    "\n",
    "Question: {q}\n",
    "Answer: {a}\n",
    "Supporting Facts: {sf}\n",
    "Graph Path: {gp}\n",
    "Type: {tp}\n",
    "Source: {src}\n",
    "Probability Yes: {prob}\n",
    "\n",
    "Task: Rewrite the \"Corrected Question\" so it is clear, self-contained, and refers to the answer and supporting facts properly.\n",
    "Corrected Question:\n",
    "\"\"\"\n",
    "    return prompt.strip()\n",
    "\n",
    "def generate_corrected_question_for_row(row, retry=1):\n",
    "    prompt = build_prompt_from_row(row)\n",
    "    try:\n",
    "        # pipeline returns list of dicts\n",
    "        out = text_gen(prompt, max_new_tokens=MAX_NEW_TOKENS, do_sample=DO_SAMPLE, temperature=TEMPERATURE)\n",
    "        gen_text = out[0][\"generated_text\"]\n",
    "        # Sometimes the pipeline returns prompt+generation; try to split\n",
    "        if \"Corrected Question:\" in gen_text:\n",
    "            gen = gen_text.split(\"Corrected Question:\")[-1].strip()\n",
    "        else:\n",
    "            # fallback: take entire generated text and remove prompt if present\n",
    "            gen = gen_text.strip()\n",
    "            if gen.startswith(prompt):\n",
    "                gen = gen[len(prompt):].strip()\n",
    "        # Post-clean common artifacts\n",
    "        # Trim to first line or sentence to keep it concise\n",
    "        gen = gen.split(\"\\n\")[0].strip()\n",
    "        # ensure not empty\n",
    "        if len(gen) == 0:\n",
    "            raise ValueError(\"Empty generation\")\n",
    "        return gen, \"\"\n",
    "    except Exception as e:\n",
    "        if retry > 0:\n",
    "            time.sleep(1.0)\n",
    "            return generate_corrected_question_for_row(row, retry=retry-1)\n",
    "        else:\n",
    "            err = f\"{type(e).__name__}: {str(e)}\"\n",
    "            return \"\", err\n",
    "\n",
    "# 5. Main loop with incremental saving\n",
    "total = len(data)\n",
    "print(\"Beginning generation for\", total, \"rows.\")\n",
    "start_idx = 0\n",
    "processed = 0\n",
    "last_save = 0\n",
    "\n",
    "for idx in range(start_idx, total):\n",
    "    # Skip if already has a result\n",
    "    if isinstance(data.at[idx, \"Qwen_Corrected_Question\"], str) and data.at[idx, \"Qwen_Corrected_Question\"].strip():\n",
    "        # already done\n",
    "        continue\n",
    "\n",
    "    row = data.iloc[idx]\n",
    "    try:\n",
    "        gen, err = generate_corrected_question_for_row(row)\n",
    "        if err:\n",
    "            print(f\"[{idx+1}/{total}] ERROR -> {err}\")\n",
    "            data.at[idx, \"Qwen_Corrected_Question\"] = \"\"\n",
    "            data.at[idx, \"Qwen_Generation_Error\"] = err\n",
    "        else:\n",
    "            data.at[idx, \"Qwen_Corrected_Question\"] = gen\n",
    "            data.at[idx, \"Qwen_Generation_Error\"] = \"\"\n",
    "            print(f\"[{idx+1}/{total}] OK -> {gen}\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Interrupted by user. Saving progress before exit...\")\n",
    "        data.to_csv(OUTPUT_FILE, index=False)\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        tb = traceback.format_exc()\n",
    "        print(f\"[{idx+1}/{total}] Unexpected error:\\n{tb}\")\n",
    "        data.at[idx, \"Qwen_Corrected_Question\"] = \"\"\n",
    "        data.at[idx, \"Qwen_Generation_Error\"] = f\"Unexpected: {type(e).__name__}\"\n",
    "\n",
    "    processed += 1\n",
    "    # Save periodically\n",
    "    if (processed - last_save) >= SAVE_EVERY:\n",
    "        print(f\"...saving progress to {OUTPUT_FILE} (processed {processed} rows)...\")\n",
    "        data.to_csv(OUTPUT_FILE, index=False)\n",
    "        last_save = processed\n",
    "\n",
    "# Final save\n",
    "data.to_csv(OUTPUT_FILE, index=False)\n",
    "print(\"All done. Results saved to:\", OUTPUT_FILE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7db57b5",
   "metadata": {},
   "source": [
    "# 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdf3841",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, logging\n",
    "\n",
    "logging.set_verbosity_error()  # suppress warnings\n",
    "\n",
    "# -----------------------\n",
    "# Config\n",
    "# -----------------------\n",
    "FINAL_DATASHEET = \"finalDataset.csv\"          # Input CSV\n",
    "OUTPUT_FILE = \"Qwen_Inference_Results.csv\"   # Output CSV\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-1.5B-Instruct\"    # Model\n",
    "MAX_NEW_TOKENS = 150\n",
    "DO_SAMPLE = False\n",
    "TEMPERATURE = 0.0\n",
    "SAVE_EVERY = 20\n",
    "# -----------------------\n",
    "\n",
    "# 0. Check file\n",
    "if not os.path.exists(FINAL_DATASHEET):\n",
    "    raise FileNotFoundError(f\"Input file not found: {FINAL_DATASHEET}\")\n",
    "\n",
    "# 1. Load dataset\n",
    "data = pd.read_csv(FINAL_DATASHEET)\n",
    "print(\"Loaded dataset:\", FINAL_DATASHEET, \"shape =\", data.shape)\n",
    "\n",
    "# 2. Device setup\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "device_str = \"cuda\" if device == 0 else \"cpu\"\n",
    "print(\"Device:\", device_str)\n",
    "\n",
    "# 3. Load model & tokenizer\n",
    "print(f\"Loading model {MODEL_NAME} ...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\" if device == 0 else \"cpu\",\n",
    "    torch_dtype=torch.float16 if device == 0 else None,\n",
    "    trust_remote_code=True,\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "text_gen = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, trust_remote_code=True)\n",
    "print(\"Model loaded.\")\n",
    "\n",
    "# 4. Prepare output columns\n",
    "if \"model_answers\" not in data.columns:\n",
    "    data[\"model_answers\"] = \"\"\n",
    "if \"Correctness\" not in data.columns:\n",
    "    data[\"Correctness\"] = \"\"\n",
    "\n",
    "# 5. Helper: build prompt to answer question\n",
    "def build_answer_prompt(row):\n",
    "    prompt = f\"\"\"\n",
    "You are a person from India with deep knowledge and lived experience of Indian culture.\n",
    "Now, answer the following question using your expertise in Indian culture by identifying the specific cultural element being referred to.\n",
    "Respond only with the name of the cultural element (e.g., Indian) — no additional text, questions, or explanations.\n",
    "\n",
    "Question: {row['question']}\n",
    "\"\"\"\n",
    "    return prompt.strip()\n",
    "\n",
    "\n",
    "# 6. Main loop\n",
    "total = len(data)\n",
    "processed = 0\n",
    "last_save = 0\n",
    "correct_count = 0\n",
    "\n",
    "for idx in range(total):\n",
    "    if isinstance(data.at[idx, \"model_answers\"], str) and data.at[idx, \"model_answers\"].strip():\n",
    "        continue  # skip already processed\n",
    "\n",
    "    row = data.iloc[idx]\n",
    "    prompt = build_answer_prompt(row)\n",
    "\n",
    "    try:\n",
    "        # Generate answer\n",
    "        out = text_gen(prompt, max_new_tokens=MAX_NEW_TOKENS, do_sample=DO_SAMPLE, temperature=TEMPERATURE)\n",
    "        gen_text = out[0][\"generated_text\"].strip()\n",
    "\n",
    "        # Optional: clean if prompt is repeated\n",
    "        if prompt in gen_text:\n",
    "            gen_text = gen_text.replace(prompt, \"\").strip()\n",
    "\n",
    "        data.at[idx, \"model_answers\"] = gen_text\n",
    "\n",
    "        # Check correctness\n",
    "        correct_answer = str(row.get(\"answer\", \"\")).strip().lower()\n",
    "        if correct_answer in gen_text.lower():\n",
    "            data.at[idx, \"Correctness\"] = True\n",
    "            correct_count += 1\n",
    "        else:\n",
    "            data.at[idx, \"Correctness\"] = False\n",
    "\n",
    "        processed += 1\n",
    "        print(f\"[{idx+1}/{total}] Answer: {gen_text} | Correct so far: {correct_count}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[{idx+1}/{total}] ERROR -> {e}\")\n",
    "        data.at[idx, \"model_answers\"] = \"\"\n",
    "        data.at[idx, \"Correctness\"] = False\n",
    "\n",
    "    # Save periodically\n",
    "    if (processed - last_save) >= SAVE_EVERY:\n",
    "        print(f\"...saving progress to {OUTPUT_FILE} (processed {processed} rows)\")\n",
    "        data.to_csv(OUTPUT_FILE, index=False)\n",
    "        last_save = processed\n",
    "\n",
    "# Final save\n",
    "data.to_csv(OUTPUT_FILE, index=False)\n",
    "print(\"Inference done. Correct answers:\", correct_count)\n",
    "print(\"Results saved to:\", OUTPUT_FILE)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

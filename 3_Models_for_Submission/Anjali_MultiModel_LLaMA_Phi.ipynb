{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af57d088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Multi-model batch inference (LLaMA + Phi) + save results\n",
    "# -------------------------------\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pandas as pd\n",
    "import gc\n",
    "import time\n",
    "import os\n",
    "\n",
    "# -------------------------------\n",
    "# 0. Memory cleanup\n",
    "# -------------------------------\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Load dataset\n",
    "# -------------------------------\n",
    "try:\n",
    "    final_datasheet = \"finalDataset.csv\"\n",
    "    data = pd.read_csv(final_datasheet, encoding=\"utf-8\")\n",
    "except UnicodeDecodeError:\n",
    "    print(\"UTF-8 failed, trying ISO-8859-1...\")\n",
    "    data = pd.read_csv(final_datasheet, encoding=\"ISO-8859-1\")\n",
    "\n",
    "# Add empty columns for storing results\n",
    "data['ModelAnswer'] = \"\"\n",
    "data['Correct'] = \"\"\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Choose model\n",
    "# -------------------------------\n",
    "# Options: \"3.1-8B-instruct\", \"3.1-8B-IT\", \"3.2-3B\", \"phi-mini\"\n",
    "MODEL_CHOICE = \"phi-mini\"  # change this variable to switch models\n",
    "\n",
    "model_dict = {\n",
    "    \"3.1-8B-instruct\": \"meta-llama/Llama-3.1-8b-instruct\",\n",
    "    \"3.1-8B-IT\": \"meta-llama/Llama-3.1-8b-IT\",\n",
    "    \"3.2-3B\": \"meta-llama/Llama-3.2-3b\",\n",
    "    \"phi-mini\": \"microsoft/Phi-4-mini-flash-reasoning\"\n",
    "}\n",
    "\n",
    "model_name = model_dict[MODEL_CHOICE]\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Load token (optional, safe check)\n",
    "# -------------------------------\n",
    "huggingface_token = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "llama_models = [\"3.1-8B-instruct\", \"3.1-8B-IT\", \"3.2-3B\"]\n",
    "\n",
    "if MODEL_CHOICE in llama_models and not huggingface_token:\n",
    "    raise ValueError(\n",
    "        f\"⚠️ The model '{MODEL_CHOICE}' requires a Hugging Face token. \"\n",
    "        \"Please set the HF_TOKEN environment variable to access it.\"\n",
    "    )\n",
    "elif huggingface_token:\n",
    "    print(\"✅ Using Hugging Face token for authentication.\")\n",
    "else:\n",
    "    print(\"ℹ️ No Hugging Face token found. Will try public model download if available (Phi-mini only).\")\n",
    "\n",
    "token_kwargs = {\"use_auth_token\": huggingface_token} if huggingface_token else {}\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Load tokenizer and model\n",
    "# -------------------------------\n",
    "print(f\"Loading tokenizer for {MODEL_CHOICE}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, **token_kwargs)\n",
    "\n",
    "print(f\"Loading model {MODEL_CHOICE} on GPU...\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    **token_kwargs,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Batch inference function\n",
    "# -------------------------------\n",
    "def generate_answer(prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=100,\n",
    "            temperature=0.0,\n",
    "            do_sample=False\n",
    "        )\n",
    "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "    return decoded.replace(prompt.strip(), \"\").strip()\n",
    "\n",
    "# -------------------------------\n",
    "# 6. Loop through dataset\n",
    "# -------------------------------\n",
    "start_time = time.time()\n",
    "total_questions = len(data)\n",
    "correct_answers = 0\n",
    "\n",
    "for idx, row in data.iterrows():\n",
    "    prompt = f\"\"\"\n",
    "You are a person from India with deep knowledge and lived experience of Indian culture.\n",
    "Now, answer the following question using your expertise in Indian culture by identifying the specific cultural element being referred to.\n",
    "Respond only with the name of the cultural element (e.g., Indian) — no additional text, questions, or explanations.\n",
    "\n",
    "Question: {row['Corrected Question']}\n",
    "\"\"\"\n",
    "    model_answer = generate_answer(prompt)\n",
    "\n",
    "    prediction_correctness = row['Answer'].strip().lower() in model_answer.lower()\n",
    "    if prediction_correctness:\n",
    "        correct_answers += 1\n",
    "\n",
    "    data.at[idx, 'ModelAnswer'] = model_answer\n",
    "    data.at[idx, 'Correct'] = str(prediction_correctness)\n",
    "\n",
    "    print(f\"Progress: {idx+1}/{total_questions} | Correct so far: {correct_answers}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 7. Summary & save\n",
    "# -------------------------------\n",
    "accuracy = (correct_answers / total_questions) * 100\n",
    "print(f\"Total correct: {correct_answers}/{total_questions}\")\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "output_file = f\"model_answers_results_{MODEL_CHOICE.replace('-', '_')}.csv\"\n",
    "data.to_csv(output_file, index=False)\n",
    "print(f\"Results saved to {output_file}\")\n",
    "print(f\"Total runtime: {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "# -------------------------------\n",
    "# 8. Cleanup\n",
    "# -------------------------------\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
